#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–çš„æ¨¡å‹è¯„ä¼°è„šæœ¬
å¿«é€Ÿæµ‹è¯•å¾®è°ƒæ•ˆæœ
"""

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

class SimpleEvaluator:
    def __init__(self):
        self.base_model_path = "fine_tuning/models/qwen3-1.7b"
        self.tuned_model_path = "fine_tuning/qwen3_economics_model"
        self.data_path = "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
    
    def load_test_questions(self):
        """åŠ è½½æµ‹è¯•é—®é¢˜"""
        test_questions = [
            "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
            "è¯·è§£é‡Šä¾›ç»™ä¸éœ€æ±‚çš„å…³ç³»",
            "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
            "è¯·è¯´æ˜ä»·æ ¼å¼¹æ€§çš„æ¦‚å¿µ",
            "ä»€ä¹ˆæ˜¯å¸‚åœºå¤±çµï¼Ÿ",
            "è§£é‡Šé€šè´§è†¨èƒ€çš„å«ä¹‰",
            "ä»€ä¹ˆæ˜¯æ¯”è¾ƒä¼˜åŠ¿ï¼Ÿ",
            "è¯·è¯´æ˜ç”Ÿäº§å¯èƒ½æ€§è¾¹ç•Œçš„æ¦‚å¿µ"
        ]
        
        # ä»æ•°æ®æ–‡ä»¶ä¸­è·å–æ ‡å‡†ç­”æ¡ˆ
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        # åŒ¹é…é—®é¢˜å’Œç­”æ¡ˆ
        qa_pairs = []
        for question in test_questions:
            for item in data:
                if question in item["question"] or item["question"] in question:
                    qa_pairs.append({
                        "question": question,
                        "reference": item["answer"]
                    })
                    break
            else:
                qa_pairs.append({
                    "question": question,
                    "reference": "æ ‡å‡†ç­”æ¡ˆæœªæ‰¾åˆ°"
                })
        
        return qa_pairs
    
    def load_models(self):
        """åŠ è½½æ¨¡å‹"""
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½å¾®è°ƒæ¨¡å‹
        print("ğŸ¯ åŠ è½½å¾®è°ƒæ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tuned_model = PeftModel.from_pretrained(base_model, self.tuned_model_path)
        tuned_model.eval()
        
        return tokenizer, tuned_model
    
    def generate_answer(self, model, tokenizer, question):
        """ç”Ÿæˆå›ç­”"""
        input_text = f"<|user|>\n{question}\n<|assistant|>\n"
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text.replace(input_text, "").strip()
            
            # æ¸…ç†ç­”æ¡ˆ
            if "\n" in answer:
                answer = answer.split("\n")[0]
            
            return answer[:200]
            
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def simple_similarity(self, ref, cand):
        """ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—"""
        ref_words = set(ref.split())
        cand_words = set(cand.split())
        
        if len(ref_words) == 0 and len(cand_words) == 0:
            return 1.0
        
        intersection = len(ref_words & cand_words)
        union = len(ref_words | cand_words)
        
        return intersection / union if union > 0 else 0.0
    
    def evaluate(self):
        """è¿è¡Œè¯„ä¼°"""
        print("ğŸ¯ ç®€åŒ–ç‰ˆå¾®è°ƒæ•ˆæœè¯„ä¼°")
        print("=" * 40)
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        qa_pairs = self.load_test_questions()
        print(f"ğŸ“š åŠ è½½äº† {len(qa_pairs)} ä¸ªæµ‹è¯•é—®é¢˜")
        
        # åŠ è½½æ¨¡å‹
        tokenizer, tuned_model = self.load_models()
        
        print("\nğŸ“Š å¼€å§‹æµ‹è¯•...")
        print("=" * 60)
        
        results = []
        total_similarity = 0
        
        for i, qa in enumerate(qa_pairs):
            question = qa["question"]
            reference = qa["reference"]
            
            print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
            print("-" * 40)
            
            # ç”Ÿæˆå›ç­”
            generated = self.generate_answer(tuned_model, tokenizer, question)
            
            # è®¡ç®—ç›¸ä¼¼åº¦
            similarity = self.simple_similarity(reference, generated)
            total_similarity += similarity
            
            print(f"ğŸ¯ æ ‡å‡†ç­”æ¡ˆ: {reference[:100]}...")
            print(f"ğŸ’¡ æ¨¡å‹å›ç­”: {generated}")
            print(f"ğŸ“ˆ ç›¸ä¼¼åº¦: {similarity:.3f}")
            
            results.append({
                "question": question,
                "reference": reference,
                "generated": generated,
                "similarity": similarity
            })
        
        # æ€»ç»“
        avg_similarity = total_similarity / len(qa_pairs)
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆ!")
        print("=" * 40)
        print(f"ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.3f}")
        
        if avg_similarity > 0.3:
            print("âœ… å¾®è°ƒæ•ˆæœè‰¯å¥½ï¼æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½åœ°å›ç­”ç»æµå­¦é—®é¢˜ã€‚")
        elif avg_similarity > 0.15:
            print("âš ï¸ å¾®è°ƒæ•ˆæœä¸€èˆ¬ï¼Œæœ‰ä¸€å®šæ”¹å–„ä½†è¿˜éœ€ä¼˜åŒ–ã€‚")
        else:
            print("âŒ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œéœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥ã€‚")
        
        # ä¿å­˜ç»“æœ
        with open("simple_evaluation_results.json", 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: simple_evaluation_results.json")
        
        return results

def main():
    evaluator = SimpleEvaluator()
    try:
        results = evaluator.evaluate()
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 