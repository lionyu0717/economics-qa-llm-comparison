#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æœ¬åœ°æ¨¡å‹è®­ç»ƒè„šæœ¬
ä½¿ç”¨å·²ç»å®‰è£…çš„GPT-2ç­‰æœ¬åœ°æ¨¡å‹è¿›è¡Œå¾®è°ƒ
é¿å…ç½‘ç»œä¸‹è½½é—®é¢˜
"""

import os
import torch
import json
import jsonlines
from datasets import Dataset
from transformers import (
    GPT2Tokenizer, 
    GPT2LMHeadModel, 
    TrainingArguments, 
    Trainer,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, TaskType, get_peft_model

def load_dataset(file_path, format_type="jsonl"):
    """åŠ è½½æ•°æ®é›†"""
    data = []
    if format_type == "jsonl":
        with jsonlines.open(file_path, 'r') as reader:
            for item in reader:
                data.append(item)
    else:  # json
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data

def create_training_text_from_qwen_format(data):
    """ä»QWENæ ¼å¼åˆ›å»ºè®­ç»ƒæ–‡æœ¬"""
    texts = []
    for item in data:
        # æå–é—®é¢˜å’Œç­”æ¡ˆ
        text = item.get("text", "")
        if "<|im_start|>user\n" in text and "<|im_start|>assistant\n" in text:
            question = text.split("<|im_start|>user\n")[1].split("<|im_end|>")[0]
            answer = text.split("<|im_start|>assistant\n")[1].split("<|im_end|>")[0]
            
            # æ ¼å¼åŒ–ä¸ºç®€å•çš„QAæ ¼å¼
            formatted_text = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š{answer}"
            texts.append(formatted_text)
    return texts

def create_training_text_from_alpaca_format(data):
    """ä»Alpacaæ ¼å¼åˆ›å»ºè®­ç»ƒæ–‡æœ¬"""
    texts = []
    for item in data:
        question = item.get("input", "")
        answer = item.get("output", "")
        
        # æ ¼å¼åŒ–ä¸ºç®€å•çš„QAæ ¼å¼
        formatted_text = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š{answer}"
        texts.append(formatted_text)
    return texts

def tokenize_function(examples, tokenizer, max_length=256):
    """tokenizeæ•°æ®"""
    model_inputs = tokenizer(
        examples["text"],
        max_length=max_length,
        truncation=True,
        padding=False,
        return_tensors=None
    )
    
    # å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼Œlabelså°±æ˜¯input_ids
    model_inputs["labels"] = model_inputs["input_ids"].copy()
    
    return model_inputs

def train_gpt2_model(data_texts, model_name="gpt2"):
    """è®­ç»ƒGPT-2æ¨¡å‹"""
    print(f"ä½¿ç”¨æ¨¡å‹: {model_name}")
    output_dir = f"./fine_tuning/local/checkpoints_{model_name.replace('/', '_')}"
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    print("åŠ è½½tokenizerå’Œæ¨¡å‹...")
    tokenizer = GPT2Tokenizer.from_pretrained(model_name)
    model = GPT2LMHeadModel.from_pretrained(model_name)
    
    # è®¾ç½®pad_token
    tokenizer.pad_token = tokenizer.eos_token
    
    # LoRAé…ç½®
    peft_config = LoraConfig(
        task_type=TaskType.CAUSAL_LM,
        inference_mode=False,
        r=4,
        lora_alpha=16,
        lora_dropout=0.1,
        target_modules=["c_attn", "c_proj"]
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()
    
    # å‡†å¤‡æ•°æ®
    train_texts = data_texts[:80]  # ä½¿ç”¨80ä¸ªæ ·æœ¬è®­ç»ƒ
    val_texts = data_texts[80:100]  # ä½¿ç”¨20ä¸ªæ ·æœ¬éªŒè¯
    
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_texts)}")
    print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_texts)}")
    
    # åˆ›å»ºDataset
    train_dataset = Dataset.from_dict({"text": train_texts})
    val_dataset = Dataset.from_dict({"text": val_texts})
    
    # Tokenize
    train_dataset = train_dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    
    val_dataset = val_dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True,
        remove_columns=["text"]
    )
    
    # æ•°æ®æ•´ç†å™¨
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        pad_to_multiple_of=8
    )
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        learning_rate=5e-5,
        fp16=True,
        logging_steps=5,
        evaluation_strategy="steps",
        eval_steps=20,
        save_strategy="steps",
        save_steps=20,
        save_total_limit=2,
        load_best_model_at_end=True,
        dataloader_num_workers=0,
        report_to=None,
    )
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("å¼€å§‹è®­ç»ƒ...")
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    print("ä¿å­˜æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    return output_dir

def test_model(model_path, tokenizer_path):
    """æµ‹è¯•å¾®è°ƒåçš„æ¨¡å‹"""
    print(f"æµ‹è¯•æ¨¡å‹: {model_path}")
    
    from peft import PeftModel
    
    # åŠ è½½æ¨¡å‹
    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)
    base_model = GPT2LMHeadModel.from_pretrained("gpt2")
    model = PeftModel.from_pretrained(base_model, model_path)
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¸‚åœºå‡è¡¡ï¼Ÿ"
    ]
    
    for question in test_questions:
        prompt = f"é—®é¢˜ï¼š{question}\nç­”æ¡ˆï¼š"
        inputs = tokenizer(prompt, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response.replace(prompt, "").strip()
        
        print(f"\né—®é¢˜: {question}")
        print(f"å›ç­”: {answer}")

def main():
    print("ğŸš€ æœ¬åœ°æ¨¡å‹å¾®è°ƒè®­ç»ƒ")
    print("="*50)
    
    # æ£€æŸ¥GPU
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs("fine_tuning/local", exist_ok=True)
    
    # åŠ è½½æ•°æ®
    print("\nåŠ è½½æ•°æ®...")
    qwen_data = load_dataset("fine_tuning/data/qwen/train.jsonl", "jsonl")
    alpaca_data = load_dataset("fine_tuning/data/alpaca/train.json", "json")
    
    # è½¬æ¢æ•°æ®æ ¼å¼
    qwen_texts = create_training_text_from_qwen_format(qwen_data)
    alpaca_texts = create_training_text_from_alpaca_format(alpaca_data)
    
    print(f"QWENæ ¼å¼æ–‡æœ¬: {len(qwen_texts)} ä¸ª")
    print(f"Alpacaæ ¼å¼æ–‡æœ¬: {len(alpaca_texts)} ä¸ª")
    
    # ä½¿ç”¨QWENæ•°æ®è®­ç»ƒç¬¬ä¸€ä¸ªæ¨¡å‹
    print("\nè®­ç»ƒæ¨¡å‹1 (åŸºäºQWENæ•°æ®)...")
    model1_path = train_gpt2_model(qwen_texts, "gpt2")
    print(f"æ¨¡å‹1ä¿å­˜åœ¨: {model1_path}")
    
    # ä½¿ç”¨Alpacaæ•°æ®è®­ç»ƒç¬¬äºŒä¸ªæ¨¡å‹
    print("\nè®­ç»ƒæ¨¡å‹2 (åŸºäºAlpacaæ•°æ®)...")
    model2_path = train_gpt2_model(alpaca_texts, "gpt2")
    print(f"æ¨¡å‹2ä¿å­˜åœ¨: {model2_path}")
    
    # æµ‹è¯•æ¨¡å‹
    print("\næµ‹è¯•æ¨¡å‹1...")
    test_model(model1_path, model1_path)
    
    print("\næµ‹è¯•æ¨¡å‹2...")
    test_model(model2_path, model2_path)
    
    print("\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print("ä¸¤ä¸ªç»æµå­¦é—®ç­”æ¨¡å‹å·²æˆåŠŸå¾®è°ƒ")

if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        print(f"è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 