#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥ä½¿ç”¨transformersåº“ä¸­çš„Gemma 3-1Bæ¨¡å‹è¿›è¡Œå¾®è°ƒ
æ— éœ€é¢„ä¸‹è½½ï¼Œä½¿ç”¨8bité‡åŒ–èŠ‚çœæ˜¾å­˜
"""

import os
import sys
import json
import time
import torch
import logging
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoTokenizer,
    Gemma3ForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Gemma3EconomicsTrainer:
    def __init__(self, 
                 model_id="google/gemma-3-1b-it",
                 data_path="ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl",
                 output_dir="fine_tuning/gemma3_economics_model",
                 hf_token="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"):
        """
        åˆå§‹åŒ–Gemma3ç»æµå­¦é—®ç­”åŠ©æ‰‹è®­ç»ƒå™¨
        
        Args:
            model_id: HuggingFaceæ¨¡å‹ID
            data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            output_dir: æ¨¡å‹è¾“å‡ºç›®å½•
            hf_token: HuggingFaceè®¿é—®token
        """
        self.model_id = model_id
        self.data_path = data_path
        self.output_dir = Path(output_dir)
        self.hf_token = hf_token
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        logger.info(f"ğŸš€ å¼€å§‹Gemma3ç»æµå­¦é—®ç­”åŠ©æ‰‹è®­ç»ƒ")
        logger.info(f"æ¨¡å‹ID: {self.model_id}")
        logger.info(f"æ•°æ®è·¯å¾„: {self.data_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_dir}")
    
    def load_model_and_tokenizer(self):
        """åŠ è½½Gemma3æ¨¡å‹å’Œtokenizerï¼Œä½¿ç”¨8bité‡åŒ–"""
        logger.info("ä»transformersåº“ç›´æ¥åŠ è½½Gemma3æ¨¡å‹...")
        
        # 8bité‡åŒ–é…ç½®ï¼ŒèŠ‚çœæ˜¾å­˜
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )
        
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_id, 
            token=self.hf_token,
            trust_remote_code=True
        )
        
        # è®¾ç½®pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        logger.info(f"TokenizeråŠ è½½æˆåŠŸï¼Œè¯æ±‡é‡: {len(self.tokenizer)}")
        
        # åŠ è½½æ¨¡å‹
        self.model = Gemma3ForCausalLM.from_pretrained(
            self.model_id,
            quantization_config=quantization_config,
            token=self.hf_token,
            torch_dtype=torch.bfloat16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # è·å–æ¨¡å‹å‚æ•°é‡
        total_params = sum(p.numel() for p in self.model.parameters())
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {total_params:,}")
        
        return self.model, self.tokenizer
    
    def setup_lora(self):
        """é…ç½®LoRAå‚æ•°"""
        logger.info("é…ç½®LoRAå‚æ•°...")
        
        # LoRAé…ç½®
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            r=16,  # rank
            lora_alpha=32,  # ç¼©æ”¾å‚æ•°
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # Gemmaçš„attentionæ¨¡å—
            bias="none",
        )
        
        # åº”ç”¨LoRA
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        logger.info("LoRAé…ç½®å®Œæˆ")
        return self.model
    
    def load_and_prepare_data(self):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        logger.info(f"åŠ è½½æ•°æ®: {self.data_path}")
        
        # è¯»å–jsonlæ–‡ä»¶
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
        
        # æ•°æ®åˆ†å‰²
        train_size = int(0.8 * len(data))
        val_size = int(0.1 * len(data))
        
        train_data = data[:train_size]
        val_data = data[train_size:train_size + val_size]
        test_data = data[train_size + val_size:]
        
        logger.info(f"è®­ç»ƒé›†: {len(train_data)} æ¡")
        logger.info(f"éªŒè¯é›†: {len(val_data)} æ¡")
        logger.info(f"æµ‹è¯•é›†: {len(test_data)} æ¡")
        
        # è½¬æ¢ä¸ºGemmaæ ¼å¼çš„å¯¹è¯
        def format_gemma_conversation(question, answer):
            """æ ¼å¼åŒ–ä¸ºGemmaçš„å¯¹è¯æ ¼å¼"""
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚"}]
                },
                {
                    "role": "user", 
                    "content": [{"type": "text", "text": question}]
                },
                {
                    "role": "assistant",
                    "content": [{"type": "text", "text": answer}]
                }
            ]
            
            # ä½¿ç”¨tokenizerçš„chat template
            formatted_text = self.tokenizer.apply_chat_template(
                [messages],
                add_generation_prompt=False,
                tokenize=False
            )[0]  # å–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œå› ä¸ºè¾“å…¥æ˜¯list of list
            
            return formatted_text
        
        # æ ¼å¼åŒ–æ•°æ®
        def process_data(data_list):
            processed = []
            for item in data_list:
                try:
                    formatted_text = format_gemma_conversation(item["question"], item["answer"])
                    processed.append({"text": formatted_text})
                except Exception as e:
                    logger.warning(f"æ ¼å¼åŒ–æ•°æ®æ—¶å‡ºé”™: {e}")
                    continue
            return processed
        
        train_formatted = process_data(train_data)
        val_formatted = process_data(val_data)
        
        # åˆ›å»ºDataset
        train_dataset = Dataset.from_list(train_formatted)
        val_dataset = Dataset.from_list(val_formatted)
        
        # Tokenizeæ•°æ®
        def tokenize_function(examples):
            # æ­£ç¡®çš„tokenization
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=True,
                max_length=512,
                return_overflowing_tokens=False,
                return_tensors=None,
            )
            
            # è®¾ç½®labelsç­‰äºinput_idsï¼ˆcausal LMçš„æ ‡å‡†åšæ³•ï¼‰
            tokenized["labels"] = tokenized["input_ids"].copy()
            
            return tokenized
        
        # åº”ç”¨tokenization
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        
        return train_dataset, val_dataset, test_data
    
    def train_model(self, train_dataset, val_dataset):
        """è®­ç»ƒæ¨¡å‹"""
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=1,  # å‡å°batch sizeï¼Œå› ä¸ºGemma3å¯èƒ½æ›´å¤§
            per_device_eval_batch_size=1,
            gradient_accumulation_steps=8,  # å¢å¤§accumulationæ¥è¡¥å¿å°batch size
            learning_rate=5e-5,
            weight_decay=0.01,
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            save_total_limit=3,
            eval_strategy="steps",
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=100,
            fp16=False,  # ä½¿ç”¨8bité‡åŒ–ï¼Œä¸éœ€è¦fp16
            bf16=True,   # ä½¿ç”¨bf16
            dataloader_drop_last=False,
            report_to=None,
            remove_unused_columns=True,
            dataloader_num_workers=0,
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        
        # åˆ›å»ºtrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            processing_class=self.tokenizer,  # ä½¿ç”¨æ–°çš„å‚æ•°å
        )
        
        # å¼€å§‹è®­ç»ƒ
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        training_time = end_time - start_time
        logger.info(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {training_time:.1f}ç§’")
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(str(self.output_dir))
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {self.output_dir}")
        
        return trainer
    
    def test_model(self, test_data, max_samples=5):
        """æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹"""
        logger.info("æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
        
        # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
        self.model.eval()
        
        test_samples = test_data[:max_samples]
        
        results = []
        for i, item in enumerate(test_samples):
            question = item["question"]
            expected_answer = item["answer"]
            
            # æ„å»ºè¾“å…¥
            messages = [
                [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚"}]
                    },
                    {
                        "role": "user",
                        "content": [{"type": "text", "text": question}]
                    }
                ]
            ]
            
            # ç”Ÿæˆå›ç­”
            try:
                inputs = self.tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt",
                ).to(self.device)
                
                with torch.inference_mode():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=128,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç è¾“å‡º
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æå–å›ç­”éƒ¨åˆ†
                if "assistant" in generated_text:
                    generated_answer = generated_text.split("assistant")[-1].strip()
                else:
                    generated_answer = generated_text.strip()
                
                result = {
                    "question": question,
                    "expected": expected_answer,
                    "generated": generated_answer
                }
                results.append(result)
                
                logger.info(f"é—®é¢˜ {i+1}: {question}")
                logger.info(f"æœŸæœ›ç­”æ¡ˆ: {expected_answer[:100]}...")
                logger.info(f"ç”Ÿæˆç­”æ¡ˆ: {generated_answer[:100]}...")
                logger.info("-" * 50)
                
            except Exception as e:
                logger.error(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
                continue
        
        return results
    
    def run_full_training(self):
        """è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹"""
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.load_model_and_tokenizer()
            
            # 2. é…ç½®LoRA
            self.setup_lora()
            
            # 3. å‡†å¤‡æ•°æ®
            train_dataset, val_dataset, test_data = self.load_and_prepare_data()
            
            # 4. è®­ç»ƒæ¨¡å‹
            trainer = self.train_model(train_dataset, val_dataset)
            
            # 5. æµ‹è¯•æ¨¡å‹
            results = self.test_model(test_data)
            
            logger.info("ğŸ‰ Gemma3ç»æµå­¦æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            raise

def main():
    print("ğŸ¤– Gemma3ç»æµå­¦é—®ç­”åŠ©æ‰‹è®­ç»ƒ")
    print("ä½¿ç”¨transformersåº“ç›´æ¥åŠ è½½Gemma3-1Bæ¨¡å‹")
    
    # æ£€æŸ¥æ–‡ä»¶
    required_files = [
        "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
    ]
    
    for file_path in required_files:
        if not Path(file_path).exists():
            print(f"âŒ ç¼ºå°‘æ–‡ä»¶: {file_path}")
            return
    
    print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹è®­ç»ƒ...")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è¿è¡Œ
    trainer = Gemma3EconomicsTrainer()
    trainer.run_full_training()

if __name__ == "__main__":
    main() 