#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•Qwen3æ¨¡å‹ - ä½¿ç”¨å®˜æ–¹æ¨èçš„æœ€ä½³å‚æ•°
è§£å†³é‡å¤å›ç­”é—®é¢˜
"""

import torch
import time
import json
import logging
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def load_qwen3_model():
    """åŠ è½½Qwen3å¾®è°ƒæ¨¡å‹"""
    try:
        print("ğŸ¤– åŠ è½½Qwen3æ¨¡å‹...")
        
        # æ¨¡å‹è·¯å¾„
        base_model_path = "fine_tuning/models/qwen3-1.7b"
        adapter_path = "fine_tuning/qwen3_economics_model"
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.bfloat16,  # ä½¿ç”¨åŸå§‹æ•°æ®ç±»å‹
            device_map="auto",
            trust_remote_code=True,
            load_in_8bit=True  # ä½¿ç”¨8bité‡åŒ–èŠ‚çœæ˜¾å­˜
        )
        
        # åŠ è½½LoRAé€‚é…å™¨
        model = PeftModel.from_pretrained(base_model, adapter_path)
        model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def generate_answer_optimized(model, tokenizer, question, mode="non_thinking"):
    """
    ä½¿ç”¨ä¼˜åŒ–å‚æ•°ç”Ÿæˆå›ç­”
    mode: "thinking" æˆ– "non_thinking"
    """
    try:
        # æ„å»ºprompt - ä½¿ç”¨Qwen3æ ‡å‡†æ ¼å¼
        if mode == "thinking":
            # ä½¿ç”¨thinkingæ¨¡å¼çš„æ ‡å‡†æ ¼å¼
            messages = [{"role": "user", "content": f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n{question}"}]
            prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True,
                enable_thinking=True
            )
        else:
            # ä½¿ç”¨ç®€å•æ ¼å¼ï¼Œéthinkingæ¨¡å¼
            prompt = f"<|user|>\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n{question}\n<|assistant|>\n"
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        # æ ¹æ®æ¨¡å¼è®¾ç½®ä¸åŒçš„ç”Ÿæˆå‚æ•°
        if mode == "thinking":
            # Thinkingæ¨¡å¼å‚æ•°ï¼ˆå®˜æ–¹æ¨èï¼‰
            generation_params = {
                "max_new_tokens": 200,
                "temperature": 0.6,
                "top_p": 0.95,
                "top_k": 20,
                "do_sample": True,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.2,  # å¢åŠ é‡å¤æƒ©ç½š
            }
        else:
            # Non-thinkingæ¨¡å¼å‚æ•°ï¼ˆå®˜æ–¹æ¨èï¼‰
            generation_params = {
                "max_new_tokens": 200,
                "temperature": 0.7,
                "top_p": 0.8,
                "top_k": 20,
                "do_sample": True,
                "pad_token_id": tokenizer.pad_token_id,
                "eos_token_id": tokenizer.eos_token_id,
                "repetition_penalty": 1.3,  # æ›´é«˜çš„é‡å¤æƒ©ç½š
            }
        
        start_time = time.time()
        
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                **generation_params
            )
        
        generation_time = time.time() - start_time
        
        # è§£ç è¾“å‡º
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–å›ç­”éƒ¨åˆ†
        if mode == "thinking":
            # å¤„ç†thinkingæ¨¡å¼çš„è¾“å‡º
            if "<think>" in full_response and "</think>" in full_response:
                # æœ‰thinkingè¿‡ç¨‹
                think_start = full_response.find("<think>")
                think_end = full_response.find("</think>") + 8
                thinking_content = full_response[think_start:think_end]
                answer = full_response[think_end:].strip()
                print(f"ğŸ’­ æ¨ç†è¿‡ç¨‹: {thinking_content[:100]}...")
            else:
                answer = full_response.replace(prompt, "").strip()
        else:
            answer = full_response.replace(prompt, "").strip()
        
        # æ¸…ç†ç­”æ¡ˆ
        if "\n" in answer:
            lines = answer.split("\n")
            # å–ç¬¬ä¸€ä¸ªéç©ºè¡Œä½œä¸ºä¸»è¦ç­”æ¡ˆ
            for line in lines:
                if line.strip():
                    answer = line.strip()
                    break
        
        # é™åˆ¶ç­”æ¡ˆé•¿åº¦
        if len(answer) > 300:
            answer = answer[:300] + "..."
        
        return answer, generation_time
        
    except Exception as e:
        return f"ç”Ÿæˆå¤±è´¥: {str(e)}", 0

def test_economics_questions():
    """æµ‹è¯•ç»æµå­¦é—®é¢˜"""
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_qwen3_model()
    if model is None:
        return
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
        "è¯·è§£é‡Šä¾›ç»™ä¸éœ€æ±‚çš„å…³ç³»",
        "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯è¾¹é™…æ•ˆç”¨ï¼Ÿ",
        "è¯·è¯´æ˜ç”Ÿäº§è¦ç´ çš„æ¦‚å¿µ"
    ]
    
    print("\nğŸ§ª å¼€å§‹æµ‹è¯•Qwen3ä¼˜åŒ–å‚æ•°...")
    print("=" * 70)
    
    results = []
    
    for mode in ["non_thinking", "thinking"]:
        print(f"\nğŸ“‹ æµ‹è¯•æ¨¡å¼: {mode.upper()}")
        print("-" * 50)
        
        mode_results = []
        
        for i, question in enumerate(test_questions):
            print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
            
            answer, gen_time = generate_answer_optimized(model, tokenizer, question, mode)
            
            print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {gen_time:.2f}ç§’")
            print(f"ğŸ’¡ å›ç­”: {answer}")
            
            mode_results.append({
                "question": question,
                "answer": answer,
                "generation_time": gen_time,
                "answer_length": len(answer)
            })
            
            print("-" * 30)
        
        results.append({
            "mode": mode,
            "results": mode_results,
            "avg_time": sum(r["generation_time"] for r in mode_results) / len(mode_results),
            "avg_length": sum(r["answer_length"] for r in mode_results) / len(mode_results)
        })
    
    # è¾“å‡ºå¯¹æ¯”æŠ¥å‘Š
    print("\nğŸ“Š å‚æ•°ä¼˜åŒ–æ•ˆæœå¯¹æ¯”")
    print("=" * 70)
    
    for result in results:
        mode = result["mode"]
        avg_time = result["avg_time"]
        avg_length = result["avg_length"]
        
        print(f"ğŸ“‹ {mode.upper()} æ¨¡å¼:")
        print(f"   â±ï¸ å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}ç§’")
        print(f"   ğŸ“ å¹³å‡å›ç­”é•¿åº¦: {avg_length:.1f}å­—ç¬¦")
        
        # æ£€æŸ¥é‡å¤é—®é¢˜
        repetition_count = 0
        for r in result["results"]:
            answer = r["answer"]
            # ç®€å•æ£€æŸ¥é‡å¤ï¼ˆç›¸åŒå¥å­å‡ºç°å¤šæ¬¡ï¼‰
            sentences = answer.split("ã€‚")
            if len(sentences) > 2:
                unique_sentences = set(sentences)
                if len(unique_sentences) < len(sentences) * 0.8:  # 80%ä»¥ä¸Šå¥å­é‡å¤
                    repetition_count += 1
        
        print(f"   ğŸ”„ é‡å¤å›ç­”æ•°é‡: {repetition_count}/{len(result['results'])}")
        print()
    
    # ä¿å­˜ç»“æœ
    report_file = f"qwen3_parameter_optimization_report_{int(time.time())}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    
    return results

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Qwen3å‚æ•°ä¼˜åŒ–æµ‹è¯•")
    print("è§£å†³é‡å¤å›ç­”é—®é¢˜ï¼Œå¯¹æ¯”thinking vs non-thinkingæ¨¡å¼")
    print("=" * 70)
    
    try:
        results = test_economics_questions()
        print("\nâœ… æµ‹è¯•å®Œæˆï¼")
        
        if results:
            print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
            print("1. ä½¿ç”¨å®˜æ–¹æ¨èçš„å‚æ•°ç»„åˆ")
            print("2. å¢åŠ repetition_penaltyåˆ°1.2-1.3")
            print("3. æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©thinkingæˆ–non-thinkingæ¨¡å¼")
            print("4. è€ƒè™‘ä½¿ç”¨presence_penaltyå‚æ•°ï¼ˆå¦‚æœæ¡†æ¶æ”¯æŒï¼‰")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 