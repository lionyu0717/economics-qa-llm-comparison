#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”è¯„ä¼°Qwen3å’ŒGemma3ä¸¤ä¸ªç»æµå­¦å¾®è°ƒæ¨¡å‹
åœ¨ç›¸åŒæµ‹è¯•é›†ä¸Šè¿›è¡Œå…¬å¹³æ¯”è¾ƒ
"""

import torch
import json
import time
import numpy as np
from pathlib import Path
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM, 
    Gemma3ForCausalLM,
    BitsAndBytesConfig
)
from peft import PeftModel
import logging
from collections import defaultdict
import pandas as pd

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ModelComparator:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.hf_token = "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"
        
        # æ¨¡å‹é…ç½®
        self.models_config = {
            "qwen3": {
                "base_model": "fine_tuning/models/qwen3-1.7b",
                "tuned_model": "fine_tuning/qwen3_economics_model",
                "model_class": AutoModelForCausalLM,
                "display_name": "Qwen3-1.7B"
            },
            "gemma3": {
                "base_model": "google/gemma-3-1b-it",
                "tuned_model": "fine_tuning/gemma3_economics_model", 
                "model_class": Gemma3ForCausalLM,
                "display_name": "Gemma3-1B"
            }
        }
        
        # æµ‹è¯•æ•°æ®è·¯å¾„
        self.test_data_path = "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
        
    def load_test_data(self, max_samples=20):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        logger.info("ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®...")
        
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            all_data = [json.loads(line) for line in f]
        
        # ä½¿ç”¨åé¢çš„æ•°æ®ä½œä¸ºæµ‹è¯•é›†ï¼ˆå‰é¢ç”¨äºè®­ç»ƒï¼‰
        train_size = int(0.8 * len(all_data))
        val_size = int(0.1 * len(all_data))
        test_data = all_data[train_size + val_size:]
        
        # é€‰æ‹©æµ‹è¯•æ ·æœ¬
        test_samples = test_data[:max_samples]
        
        logger.info(f"åŠ è½½äº† {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_samples
    
    def load_model(self, model_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        config = self.models_config[model_name]
        logger.info(f"ğŸ”„ åŠ è½½ {config['display_name']} æ¨¡å‹...")
        
        try:
            # 8bité‡åŒ–é…ç½®
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
            )
            
            # åŠ è½½tokenizer
            if model_name == "qwen3":
                tokenizer = AutoTokenizer.from_pretrained(
                    config["base_model"], 
                    trust_remote_code=True
                )
            else:  # gemma3
                tokenizer = AutoTokenizer.from_pretrained(
                    config["base_model"], 
                    token=self.hf_token,
                    trust_remote_code=True
                )
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            if model_name == "qwen3":
                base_model = config["model_class"].from_pretrained(
                    config["base_model"],
                    quantization_config=quantization_config,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:  # gemma3
                base_model = config["model_class"].from_pretrained(
                    config["base_model"],
                    quantization_config=quantization_config,
                    token=self.hf_token,
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    trust_remote_code=True
                )
            
            # åŠ è½½LoRAé€‚é…å™¨
            model = PeftModel.from_pretrained(base_model, config["tuned_model"])
            model.eval()
            
            logger.info(f"âœ… {config['display_name']} æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ {config['display_name']} æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def generate_answer(self, model, tokenizer, question, model_name):
        """ç”Ÿæˆæ¨¡å‹å›ç­”"""
        try:
            if model_name == "qwen3":
                # Qwen3æ ¼å¼
                prompt = f"<|user|>\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n{question}\n<|assistant|>\n"
                
                inputs = tokenizer.encode(prompt, return_tensors="pt").to(self.device)
                
                with torch.inference_mode():
                    outputs = model.generate(
                        inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id,
                        eos_token_id=tokenizer.eos_token_id
                    )
                
                full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                answer = full_response.replace(prompt, "").strip()
                
            else:  # gemma3
                # Gemma3æ ¼å¼
                messages = [
                    [
                        {
                            "role": "system",
                            "content": [{"type": "text", "text": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚"}]
                        },
                        {
                            "role": "user",
                            "content": [{"type": "text", "text": question}]
                        }
                    ]
                ]
                
                inputs = tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    tokenize=True,
                    return_dict=True,
                    return_tensors="pt",
                ).to(self.device)
                
                with torch.inference_mode():
                    outputs = model.generate(
                        **inputs,
                        max_new_tokens=150,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=tokenizer.eos_token_id
                    )
                
                full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                
                # æå–assistantçš„å›ç­”
                if "assistant" in full_response:
                    answer = full_response.split("assistant")[-1].strip()
                else:
                    answer = full_response.strip()
            
            return answer
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return f"[ç”Ÿæˆå¤±è´¥: {str(e)}]"
    
    def evaluate_model(self, model_name, test_data):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        logger.info(f"ğŸ“Š è¯„ä¼° {self.models_config[model_name]['display_name']} æ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹
        model, tokenizer = self.load_model(model_name)
        if model is None:
            return None
        
        results = []
        total_time = 0
        
        for i, item in enumerate(test_data):
            question = item["question"]
            expected_answer = item["answer"]
            
            logger.info(f"ğŸ”¸ å¤„ç†é—®é¢˜ {i+1}/{len(test_data)}: {question[:50]}...")
            
            # ç”Ÿæˆå›ç­”å¹¶è®¡æ—¶
            start_time = time.time()
            generated_answer = self.generate_answer(model, tokenizer, question, model_name)
            end_time = time.time()
            
            generation_time = end_time - start_time
            total_time += generation_time
            
            result = {
                "question": question,
                "expected": expected_answer,
                "generated": generated_answer,
                "time": generation_time,
                "length": len(generated_answer)
            }
            results.append(result)
            
            logger.info(f"   â±ï¸ è€—æ—¶: {generation_time:.2f}s")
            logger.info(f"   ğŸ“ å›ç­”: {generated_answer[:100]}...")
        
        # æ¸…ç†GPUå†…å­˜
        del model, tokenizer
        torch.cuda.empty_cache()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_time = total_time / len(test_data)
        avg_length = np.mean([r["length"] for r in results])
        
        model_stats = {
            "model_name": self.models_config[model_name]['display_name'],
            "total_time": total_time,
            "avg_time": avg_time,
            "avg_length": avg_length,
            "results": results
        }
        
        logger.info(f"âœ… {self.models_config[model_name]['display_name']} è¯„ä¼°å®Œæˆ")
        logger.info(f"   ğŸ“Š å¹³å‡è€—æ—¶: {avg_time:.2f}s")
        logger.info(f"   ğŸ“ å¹³å‡é•¿åº¦: {avg_length:.1f}å­—ç¬¦")
        
        return model_stats
    
    def compare_answers(self, qwen_stats, gemma_stats):
        """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„å›ç­”"""
        logger.info("ğŸ” åˆ†æå›ç­”è´¨é‡...")
        
        comparison_results = []
        
        for i in range(len(qwen_stats["results"])):
            qwen_result = qwen_stats["results"][i]
            gemma_result = gemma_stats["results"][i]
            
            comparison = {
                "question": qwen_result["question"],
                "expected": qwen_result["expected"],
                "qwen_answer": qwen_result["generated"],
                "gemma_answer": gemma_result["generated"],
                "qwen_time": qwen_result["time"],
                "gemma_time": gemma_result["time"],
                "qwen_length": qwen_result["length"],
                "gemma_length": gemma_result["length"]
            }
            comparison_results.append(comparison)
        
        return comparison_results
    
    def generate_report(self, qwen_stats, gemma_stats, comparisons):
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        logger.info("ğŸ“ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        report = []
        report.append("ğŸ† Qwen3 vs Gemma3 ç»æµå­¦é—®ç­”æ¨¡å‹å¯¹æ¯”æŠ¥å‘Š")
        report.append("=" * 60)
        report.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report.append("ğŸ“Š åŸºæœ¬æ€§èƒ½ç»Ÿè®¡:")
        report.append(f"â”œâ”€ Qwen3-1.7B:")
        report.append(f"â”‚  â”œâ”€ å¹³å‡å“åº”æ—¶é—´: {qwen_stats['avg_time']:.2f}s")
        report.append(f"â”‚  â””â”€ å¹³å‡å›ç­”é•¿åº¦: {qwen_stats['avg_length']:.1f}å­—ç¬¦")
        report.append(f"â””â”€ Gemma3-1B:")
        report.append(f"   â”œâ”€ å¹³å‡å“åº”æ—¶é—´: {gemma_stats['avg_time']:.2f}s")
        report.append(f"   â””â”€ å¹³å‡å›ç­”é•¿åº¦: {gemma_stats['avg_length']:.1f}å­—ç¬¦")
        report.append("")
        
        # æ€§èƒ½å¯¹æ¯”
        speed_winner = "Qwen3" if qwen_stats['avg_time'] < gemma_stats['avg_time'] else "Gemma3"
        speed_diff = abs(qwen_stats['avg_time'] - gemma_stats['avg_time'])
        
        report.append("âš¡ æ€§èƒ½å¯¹æ¯”:")
        report.append(f"â”œâ”€ å“åº”é€Ÿåº¦ä¼˜èƒœ: {speed_winner} (å¿« {speed_diff:.2f}s)")
        report.append("")
        
        # è¯¦ç»†å¯¹æ¯”
        report.append("ğŸ” è¯¦ç»†å›ç­”å¯¹æ¯”:")
        report.append("-" * 60)
        
        for i, comp in enumerate(comparisons):  # æ˜¾ç¤ºæ‰€æœ‰é—®é¢˜
            report.append(f"\né—®é¢˜ {i+1}: {comp['question']}")
            report.append(f"æœŸæœ›ç­”æ¡ˆ: {comp['expected'][:100]}...")
            report.append("")
            report.append(f"ğŸŸ¦ Qwen3å›ç­” ({comp['qwen_time']:.2f}s, {comp['qwen_length']}å­—ç¬¦):")
            report.append(f"   {comp['qwen_answer'][:200]}...")
            report.append("")
            report.append(f"ğŸŸ¨ Gemma3å›ç­” ({comp['gemma_time']:.2f}s, {comp['gemma_length']}å­—ç¬¦):")
            report.append(f"   {comp['gemma_answer'][:200]}...")
            report.append("-" * 40)
        
        # ä¿å­˜æŠ¥å‘Š
        report_text = "\n".join(report)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        report_file = f"model_comparison_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_text)
        
        logger.info(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ† æ¨¡å‹å¯¹æ¯”ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"âš¡ å“åº”é€Ÿåº¦: {speed_winner} è·èƒœ (å¿« {speed_diff:.2f}ç§’)")
        print(f"ğŸ“ å›ç­”é•¿åº¦: Qwen3å¹³å‡{qwen_stats['avg_length']:.0f}å­—ç¬¦, Gemma3å¹³å‡{gemma_stats['avg_length']:.0f}å­—ç¬¦")
        print(f"ğŸ“Š è¯¦ç»†å¯¹æ¯”è¯·æŸ¥çœ‹: {report_file}")
        print("="*60)
        
        return report_file
    
    def run_comparison(self, max_samples=10):
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹å¯¹æ¯”"""
        logger.info("ğŸš€ å¼€å§‹æ¨¡å‹å¯¹æ¯”è¯„ä¼°...")
        print("\nğŸ¤– Qwen3 vs Gemma3 ç»æµå­¦é—®ç­”æ¨¡å‹å¯¹æ¯”")
        print("=" * 50)
        
        try:
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = self.load_test_data(max_samples)
            
            # è¯„ä¼°Qwen3
            print(f"\nğŸ”µ æ­£åœ¨è¯„ä¼° Qwen3-1.7B æ¨¡å‹...")
            qwen_stats = self.evaluate_model("qwen3", test_data)
            if qwen_stats is None:
                logger.error("Qwen3æ¨¡å‹è¯„ä¼°å¤±è´¥")
                return
            
            # è¯„ä¼°Gemma3
            print(f"\nğŸŸ¡ æ­£åœ¨è¯„ä¼° Gemma3-1B æ¨¡å‹...")
            gemma_stats = self.evaluate_model("gemma3", test_data)
            if gemma_stats is None:
                logger.error("Gemma3æ¨¡å‹è¯„ä¼°å¤±è´¥")
                return
            
            # å¯¹æ¯”åˆ†æ
            print(f"\nğŸ” å¯¹æ¯”åˆ†æç»“æœ...")
            comparisons = self.compare_answers(qwen_stats, gemma_stats)
            
            # ç”ŸæˆæŠ¥å‘Š
            report_file = self.generate_report(qwen_stats, gemma_stats, comparisons)
            
            logger.info("ğŸ‰ æ¨¡å‹å¯¹æ¯”è¯„ä¼°å®Œæˆï¼")
            return report_file
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
            raise

def main():
    print("ğŸ† ç»æµå­¦é—®ç­”æ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("å¯¹æ¯” Qwen3-1.7B vs Gemma3-1B å¾®è°ƒæ¨¡å‹")
    print("=" * 50)
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_paths = [
        "fine_tuning/qwen3_economics_model",
        "fine_tuning/gemma3_economics_model",
        "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
    ]
    
    for path in required_paths:
        if not Path(path).exists():
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {path}")
            return
    
    print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # å¼€å§‹å¯¹æ¯”
    comparator = ModelComparator()
    report_file = comparator.run_comparison(max_samples=10)  # å…ˆç”¨10ä¸ªæ ·æœ¬æµ‹è¯•
    
    print(f"\nğŸŠ è¯„ä¼°å®Œæˆï¼æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {report_file}")

if __name__ == "__main__":
    main() 