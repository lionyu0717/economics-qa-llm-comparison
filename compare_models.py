#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¯¹æ¯”å¾®è°ƒå‰åæ¨¡å‹çš„å›ç­”è´¨é‡
æ›´ç›´è§‚åœ°è¯„ä¼°å¾®è°ƒæ•ˆæœ
"""

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time

class ModelComparator:
    def __init__(self):
        self.base_model_path = "fine_tuning/models/qwen3-1.7b"
        self.tuned_model_path = "fine_tuning/qwen3_economics_model"
        
    def load_models(self):
        """åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        print("ğŸ“¦ åŠ è½½åŸå§‹æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        base_model.eval()
        
        # åŠ è½½å¾®è°ƒæ¨¡å‹
        print("ğŸ¯ åŠ è½½å¾®è°ƒæ¨¡å‹...")
        tuned_base = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto", 
            trust_remote_code=True
        )
        tuned_model = PeftModel.from_pretrained(tuned_base, self.tuned_model_path)
        tuned_model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
        return tokenizer, base_model, tuned_model
    
    def generate_answer(self, model, tokenizer, question, model_name=""):
        """ç”Ÿæˆå›ç­”"""
        input_text = f"<|user|>\n{question}\n<|assistant|>\n"
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generation_time = time.time() - start_time
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text.replace(input_text, "").strip()
            
            # æ¸…ç†ç­”æ¡ˆ
            if "\n" in answer:
                answer = answer.split("\n")[0]
            
            return answer[:300], generation_time
            
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}", 0
    
    def get_test_questions(self):
        """è·å–æµ‹è¯•é—®é¢˜"""
        return [
            "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
            "è¯·è§£é‡Šä¾›ç»™ä¸éœ€æ±‚çš„å…³ç³»", 
            "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
            "è¯·è¯´æ˜ä»·æ ¼å¼¹æ€§çš„æ¦‚å¿µ",
            "ä»€ä¹ˆæ˜¯å¸‚åœºå¤±çµï¼Ÿ",
            "è§£é‡Šé€šè´§è†¨èƒ€çš„å«ä¹‰",
            "ä»€ä¹ˆæ˜¯æ¯”è¾ƒä¼˜åŠ¿ï¼Ÿ",
            "è¯·è¯´æ˜ç”Ÿäº§å¯èƒ½æ€§è¾¹ç•Œçš„æ¦‚å¿µ",
            "ä»€ä¹ˆæ˜¯è¾¹é™…æ•ˆç”¨ï¼Ÿ",
            "è§£é‡Šç»æµå­¦ä¸­çš„æ•ˆç‡æ¦‚å¿µ"
        ]
    
    def compare_models(self):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
        print("ğŸ¯ å¾®è°ƒå‰åæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
        print("=" * 60)
        
        # åŠ è½½æ¨¡å‹
        tokenizer, base_model, tuned_model = self.load_models()
        
        # è·å–æµ‹è¯•é—®é¢˜
        questions = self.get_test_questions()
        
        print(f"\nğŸ“ å¼€å§‹å¯¹æ¯”æµ‹è¯• ({len(questions)} ä¸ªé—®é¢˜)...")
        print("=" * 60)
        
        results = []
        
        for i, question in enumerate(questions):
            print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
            print("=" * 50)
            
            # åŸå§‹æ¨¡å‹å›ç­”
            print("ğŸ“¦ åŸå§‹æ¨¡å‹å›ç­”:")
            base_answer, base_time = self.generate_answer(base_model, tokenizer, question, "åŸå§‹")
            print(f"ğŸ’­ {base_answer}")
            print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {base_time:.2f}ç§’")
            
            print("\n" + "-" * 50)
            
            # å¾®è°ƒæ¨¡å‹å›ç­”
            print("ğŸ¯ å¾®è°ƒæ¨¡å‹å›ç­”:")
            tuned_answer, tuned_time = self.generate_answer(tuned_model, tokenizer, question, "å¾®è°ƒ")
            print(f"ğŸ’¡ {tuned_answer}")
            print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {tuned_time:.2f}ç§’")
            
            # ä¿å­˜ç»“æœ
            results.append({
                "question": question,
                "base_answer": base_answer,
                "tuned_answer": tuned_answer,
                "base_time": base_time,
                "tuned_time": tuned_time
            })
            
            print("\n" + "="*60)
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        self.save_comparison_results(results)
        
        # æ˜¾ç¤ºæ€»ç»“
        self.print_summary(results)
        
        return results
    
    def save_comparison_results(self, results):
        """ä¿å­˜å¯¹æ¯”ç»“æœ"""
        output_file = "model_comparison_results.json"
        
        comparison_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "total_questions": len(results),
            "results": results,
            "summary": self.calculate_summary(results)
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ å¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # åˆ›å»ºå¯è¯»çš„æŠ¥å‘Š
        self.create_readable_report(results)
    
    def calculate_summary(self, results):
        """è®¡ç®—æ€»ç»“ç»Ÿè®¡"""
        base_times = [r["base_time"] for r in results if r["base_time"] > 0]
        tuned_times = [r["tuned_time"] for r in results if r["tuned_time"] > 0]
        
        base_avg_len = sum(len(r["base_answer"]) for r in results) / len(results)
        tuned_avg_len = sum(len(r["tuned_answer"]) for r in results) / len(results)
        
        return {
            "base_avg_time": sum(base_times) / len(base_times) if base_times else 0,
            "tuned_avg_time": sum(tuned_times) / len(tuned_times) if tuned_times else 0,
            "base_avg_length": base_avg_len,
            "tuned_avg_length": tuned_avg_len
        }
    
    def create_readable_report(self, results):
        """åˆ›å»ºå¯è¯»çš„å¯¹æ¯”æŠ¥å‘Š"""
        report_file = "model_comparison_report.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("ğŸ¯ Qwen3 ç»æµå­¦æ¨¡å‹å¾®è°ƒå‰åå¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æµ‹è¯•é—®é¢˜æ•°é‡: {len(results)}\n\n")
            
            for i, result in enumerate(results):
                f.write(f"é—®é¢˜ {i+1}: {result['question']}\n")
                f.write("-" * 50 + "\n")
                f.write(f"åŸå§‹æ¨¡å‹: {result['base_answer']}\n\n")
                f.write(f"å¾®è°ƒæ¨¡å‹: {result['tuned_answer']}\n\n")
                f.write("=" * 60 + "\n\n")
            
            # æ€»ç»“éƒ¨åˆ†
            summary = self.calculate_summary(results)
            f.write("ğŸ“Š å¯¹æ¯”æ€»ç»“:\n")
            f.write("-" * 30 + "\n")
            f.write(f"åŸå§‹æ¨¡å‹å¹³å‡å›ç­”æ—¶é—´: {summary['base_avg_time']:.2f}ç§’\n")
            f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡å›ç­”æ—¶é—´: {summary['tuned_avg_time']:.2f}ç§’\n")
            f.write(f"åŸå§‹æ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {summary['base_avg_length']:.1f}å­—ç¬¦\n")
            f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {summary['tuned_avg_length']:.1f}å­—ç¬¦\n")
        
        print(f"ğŸ“‹ å¯è¯»æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    def print_summary(self, results):
        """æ‰“å°æ€»ç»“"""
        summary = self.calculate_summary(results)
        
        print("\nğŸ‰ å¯¹æ¯”è¯„ä¼°å®Œæˆ!")
        print("=" * 40)
        print("ğŸ“Š å¿«é€Ÿæ€»ç»“:")
        print(f"  æµ‹è¯•é—®é¢˜: {len(results)} ä¸ª")
        print(f"  åŸå§‹æ¨¡å‹å¹³å‡ç”¨æ—¶: {summary['base_avg_time']:.2f}ç§’")
        print(f"  å¾®è°ƒæ¨¡å‹å¹³å‡ç”¨æ—¶: {summary['tuned_avg_time']:.2f}ç§’")
        print(f"  åŸå§‹æ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {summary['base_avg_length']:.1f}å­—ç¬¦")
        print(f"  å¾®è°ƒæ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {summary['tuned_avg_length']:.1f}å­—ç¬¦")
        
        print("\nğŸ’¡ è¯„ä¼°å»ºè®®:")
        print("  1. æŸ¥çœ‹ model_comparison_report.txt äº†è§£è¯¦ç»†å¯¹æ¯”")
        print("  2. å…³æ³¨å¾®è°ƒæ¨¡å‹æ˜¯å¦æ›´ä¸“ä¸šã€å‡†ç¡®")
        print("  3. è§‚å¯Ÿå›ç­”çš„è¿è´¯æ€§å’Œç›¸å…³æ€§")
        print("  4. æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„è¿‡æ‹Ÿåˆç°è±¡")
        
        # ç»™å‡ºç®€å•è¯„ä»·
        if summary['tuned_avg_length'] > summary['base_avg_length'] * 1.2:
            print("\nâœ… å¾®è°ƒæ¨¡å‹å›ç­”æ›´è¯¦ç»†ï¼Œå¯èƒ½å­¦åˆ°äº†æ›´å¤šç»æµå­¦çŸ¥è¯†")
        elif summary['tuned_avg_length'] < summary['base_avg_length'] * 0.8:
            print("\nâš ï¸ å¾®è°ƒæ¨¡å‹å›ç­”è¾ƒçŸ­ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°")
        else:
            print("\nğŸ” éœ€è¦äººå·¥è¯„ä¼°å›ç­”è´¨é‡æ¥åˆ¤æ–­å¾®è°ƒæ•ˆæœ")

def main():
    print("ğŸ¯ æ¨¡å‹å¯¹æ¯”è¯„ä¼°å·¥å…·")
    print("å¯¹æ¯”å¾®è°ƒå‰åæ¨¡å‹åœ¨ç»æµå­¦é—®é¢˜ä¸Šçš„è¡¨ç°")
    print("=" * 50)
    
    comparator = ModelComparator()
    
    try:
        results = comparator.compare_models()
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼å…±æµ‹è¯•äº† {len(results)} ä¸ªé—®é¢˜")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 