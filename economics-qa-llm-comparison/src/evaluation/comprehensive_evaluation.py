#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»¼åˆè¯„ä¼°Qwen3ç»æµå­¦æ¨¡å‹å¾®è°ƒæ•ˆæœ
åŒ…å«å¤šç§è¯„ä¼°æŒ‡æ ‡å’Œå¯¹æ¯”åˆ†æ
"""

import torch
import json
import numpy as np
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import jieba
from collections import defaultdict
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# å®‰è£…å¿…è¦çš„åŒ…
try:
    import nltk
    nltk.download('punkt', quiet=True)
except:
    pass

class EconomicsModelEvaluator:
    def __init__(self):
        self.base_model_path = "fine_tuning/models/qwen3-1.7b"
        self.tuned_model_path = "fine_tuning/qwen3_economics_model"
        self.data_path = "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
        
        self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
        self.smoothing = SmoothingFunction().method1
        
        # è¯„ä¼°ç»“æœ
        self.results = {
            'base_model': defaultdict(list),
            'tuned_model': defaultdict(list)
        }
    
    def load_test_data(self, max_samples=50):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        print("ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®...")
        
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = [json.loads(line) for line in f]
        
        # ä½¿ç”¨å20%ä½œä¸ºæµ‹è¯•é›†
        test_start = int(len(data) * 0.8)
        test_data = data[test_start:test_start + max_samples]
        
        processed_data = []
        for item in test_data:
            # åŸå§‹æ•°æ®æ ¼å¼æ˜¯ {"question": "...", "answer": "..."}
            question = item["question"]
            reference_answer = item["answer"]
            processed_data.append({
                "question": question,
                "reference": reference_answer
            })
        
        print(f"âœ… åŠ è½½äº† {len(processed_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return processed_data
    
    def load_models(self):
        """åŠ è½½åŸå§‹æ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹"""
        print("ğŸ¤– åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸå§‹æ¨¡å‹
        print("ğŸ“¦ åŠ è½½åŸå§‹æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        base_model.eval()
        
        # åŠ è½½å¾®è°ƒæ¨¡å‹
        print("ğŸ¯ åŠ è½½å¾®è°ƒæ¨¡å‹...")
        tuned_base = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tuned_model = PeftModel.from_pretrained(tuned_base, self.tuned_model_path)
        tuned_model.eval()
        
        return tokenizer, base_model, tuned_model
    
    def generate_answer(self, model, tokenizer, question, max_new_tokens=150):
        """ç”Ÿæˆå›ç­”"""
        input_text = f"<|user|>\n{question}\n<|assistant|>\n"
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text.replace(input_text, "").strip()
            
            # æ¸…ç†ç­”æ¡ˆï¼Œç§»é™¤é‡å¤æˆ–æ— å…³å†…å®¹
            if "\n" in answer:
                answer = answer.split("\n")[0]
            
            return answer[:300]  # é™åˆ¶é•¿åº¦
            
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def calculate_rouge_scores(self, reference, candidate):
        """è®¡ç®—ROUGEåˆ†æ•°"""
        scores = self.rouge_scorer.score(reference, candidate)
        return {
            'rouge1_f': scores['rouge1'].fmeasure,
            'rouge2_f': scores['rouge2'].fmeasure,
            'rougeL_f': scores['rougeL'].fmeasure
        }
    
    def calculate_bleu_score(self, reference, candidate):
        """è®¡ç®—BLEUåˆ†æ•°"""
        try:
            # ä½¿ç”¨jiebaåˆ†è¯
            ref_tokens = list(jieba.cut(reference))
            cand_tokens = list(jieba.cut(candidate))
            
            if len(cand_tokens) == 0:
                return 0.0
            
            return sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing)
        except:
            return 0.0
    
    def calculate_semantic_similarity(self, reference, candidate):
        """è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è®¡ç®—è¯æ±‡é‡å åº¦
        ref_words = set(jieba.cut(reference))
        cand_words = set(jieba.cut(candidate))
        
        if len(ref_words) == 0 and len(cand_words) == 0:
            return 1.0
        
        intersection = len(ref_words & cand_words)
        union = len(ref_words | cand_words)
        
        return intersection / union if union > 0 else 0.0
    
    def assess_answer_quality(self, question, reference, candidate):
        """è¯„ä¼°å›ç­”è´¨é‡ï¼ˆå¤šç»´åº¦ï¼‰"""
        
        # 1. é•¿åº¦åˆç†æ€§ (0-1)
        ref_len = len(reference)
        cand_len = len(candidate)
        length_ratio = min(cand_len / max(ref_len, 1), 1.0) if cand_len > 0 else 0.0
        
        # 2. æ˜¯å¦å›ç­”äº†é—®é¢˜ï¼ˆæ£€æŸ¥å…³é”®è¯ï¼‰
        question_keywords = set(jieba.cut(question))
        answer_keywords = set(jieba.cut(candidate))
        keyword_coverage = len(question_keywords & answer_keywords) / max(len(question_keywords), 1)
        
        # 3. æµç•…æ€§ï¼ˆç®€åŒ–è¯„ä¼°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾é”™è¯¯ï¼‰
        fluency = 1.0
        if "ç”Ÿæˆå¤±è´¥" in candidate or len(candidate) < 10:
            fluency = 0.0
        elif candidate.count("ã€‚") == 0 and len(candidate) > 50:  # é•¿æ–‡æœ¬æ²¡æœ‰å¥å·
            fluency = 0.7
        
        return {
            'length_appropriateness': length_ratio,
            'keyword_coverage': keyword_coverage,
            'fluency': fluency,
            'overall_quality': (length_ratio + keyword_coverage + fluency) / 3
        }
    
    def evaluate_model(self, model, tokenizer, test_data, model_name):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        print(f"ğŸ“Š è¯„ä¼°{model_name}...")
        
        all_results = []
        
        for i, item in enumerate(test_data):
            if i % 10 == 0:
                print(f"  è¿›åº¦: {i+1}/{len(test_data)}")
            
            question = item["question"]
            reference = item["reference"]
            
            # ç”Ÿæˆå›ç­”
            candidate = self.generate_answer(model, tokenizer, question)
            
            # è®¡ç®—å„ç§æŒ‡æ ‡
            rouge_scores = self.calculate_rouge_scores(reference, candidate)
            bleu_score = self.calculate_bleu_score(reference, candidate)
            semantic_sim = self.calculate_semantic_similarity(reference, candidate)
            quality_scores = self.assess_answer_quality(question, reference, candidate)
            
            result = {
                'question': question,
                'reference': reference,
                'candidate': candidate,
                'rouge1_f': rouge_scores['rouge1_f'],
                'rouge2_f': rouge_scores['rouge2_f'],
                'rougeL_f': rouge_scores['rougeL_f'],
                'bleu': bleu_score,
                'semantic_similarity': semantic_sim,
                **quality_scores
            }
            
            all_results.append(result)
        
        return all_results
    
    def compare_models(self, base_results, tuned_results):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹çš„æ€§èƒ½"""
        print("ğŸ” å¯¹æ¯”åˆ†æ...")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        metrics = ['rouge1_f', 'rouge2_f', 'rougeL_f', 'bleu', 'semantic_similarity', 'overall_quality']
        
        comparison = {}
        for metric in metrics:
            base_avg = np.mean([r[metric] for r in base_results])
            tuned_avg = np.mean([r[metric] for r in tuned_results])
            improvement = (tuned_avg - base_avg) / base_avg * 100 if base_avg > 0 else 0
            
            comparison[metric] = {
                'base_model': base_avg,
                'tuned_model': tuned_avg,
                'improvement_pct': improvement
            }
        
        return comparison
    
    def save_detailed_results(self, base_results, tuned_results, comparison):
        """ä¿å­˜è¯¦ç»†ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        detailed_results = {
            'timestamp': timestamp,
            'base_model_results': base_results,
            'tuned_model_results': tuned_results,
            'comparison': comparison
        }
        
        output_file = f"evaluation_results_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # åˆ›å»ºç®€è¦æŠ¥å‘Š
        self.create_summary_report(comparison, output_file.replace('.json', '_summary.txt'))
        
        return output_file
    
    def create_summary_report(self, comparison, output_file):
        """åˆ›å»ºç®€è¦è¯„ä¼°æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("ğŸ¯ Qwen3ç»æµå­¦æ¨¡å‹å¾®è°ƒæ•ˆæœè¯„ä¼°æŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("ğŸ“Š ä¸»è¦æŒ‡æ ‡å¯¹æ¯”:\n")
            f.write("-" * 40 + "\n")
            
            metrics_chinese = {
                'rouge1_f': 'ROUGE-1 (è¯æ±‡é‡å )',
                'rouge2_f': 'ROUGE-2 (çŸ­è¯­é‡å )', 
                'rougeL_f': 'ROUGE-L (æœ€é•¿å…¬å…±å­åºåˆ—)',
                'bleu': 'BLEU (ç¿»è¯‘è¯„ä¼°)',
                'semantic_similarity': 'è¯­ä¹‰ç›¸ä¼¼åº¦',
                'overall_quality': 'ç»¼åˆè´¨é‡è¯„åˆ†'
            }
            
            for metric, data in comparison.items():
                if metric in metrics_chinese:
                    name = metrics_chinese[metric]
                    f.write(f"{name}:\n")
                    f.write(f"  åŸå§‹æ¨¡å‹: {data['base_model']:.4f}\n")
                    f.write(f"  å¾®è°ƒæ¨¡å‹: {data['tuned_model']:.4f}\n")
                    f.write(f"  æå‡å¹…åº¦: {data['improvement_pct']:+.2f}%\n\n")
            
            # è¯„ä¼°ç»“è®º
            avg_improvement = np.mean([data['improvement_pct'] for data in comparison.values()])
            f.write("ğŸ‰ è¯„ä¼°ç»“è®º:\n")
            f.write("-" * 40 + "\n")
            
            if avg_improvement > 10:
                f.write("âœ… å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼æ¨¡å‹åœ¨ç»æµå­¦é—®ç­”æ–¹é¢æœ‰æ˜æ˜¾æå‡ã€‚\n")
            elif avg_improvement > 5:
                f.write("âœ… å¾®è°ƒæ•ˆæœè‰¯å¥½ï¼Œæ¨¡å‹æ€§èƒ½æœ‰æ‰€æ”¹å–„ã€‚\n")
            elif avg_improvement > 0:
                f.write("âš ï¸ å¾®è°ƒæ•ˆæœä¸€èˆ¬ï¼Œæœ‰è½»å¾®æå‡ã€‚\n")
            else:
                f.write("âŒ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥ã€‚\n")
            
            f.write(f"å¹³å‡æå‡å¹…åº¦: {avg_improvement:+.2f}%\n")
        
        print(f"ğŸ“‹ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def run_comprehensive_evaluation(self, max_samples=30):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        print("ğŸš€ å¼€å§‹ç»¼åˆè¯„ä¼°...")
        print("=" * 50)
        
        # 1. åŠ è½½æµ‹è¯•æ•°æ®
        test_data = self.load_test_data(max_samples)
        
        # 2. åŠ è½½æ¨¡å‹
        tokenizer, base_model, tuned_model = self.load_models()
        
        # 3. è¯„ä¼°åŸå§‹æ¨¡å‹
        base_results = self.evaluate_model(base_model, tokenizer, test_data, "åŸå§‹æ¨¡å‹")
        
        # 4. è¯„ä¼°å¾®è°ƒæ¨¡å‹
        tuned_results = self.evaluate_model(tuned_model, tokenizer, test_data, "å¾®è°ƒæ¨¡å‹")
        
        # 5. å¯¹æ¯”åˆ†æ
        comparison = self.compare_models(base_results, tuned_results)
        
        # 6. ä¿å­˜ç»“æœ
        output_file = self.save_detailed_results(base_results, tuned_results, comparison)
        
        # 7. æ˜¾ç¤ºæ‘˜è¦
        self.print_summary(comparison)
        
        return output_file
    
    def print_summary(self, comparison):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\nğŸ¯ è¯„ä¼°ç»“æœæ‘˜è¦")
        print("=" * 40)
        
        for metric, data in comparison.items():
            if metric in ['rouge1_f', 'bleu', 'overall_quality']:
                print(f"{metric.upper()}:")
                print(f"  åŸå§‹: {data['base_model']:.4f}")
                print(f"  å¾®è°ƒ: {data['tuned_model']:.4f}")
                print(f"  æå‡: {data['improvement_pct']:+.2f}%")
                print()

def main():
    print("ğŸ¯ Qwen3ç»æµå­¦æ¨¡å‹ç»¼åˆè¯„ä¼°")
    print("=" * 50)
    
    evaluator = EconomicsModelEvaluator()
    
    try:
        output_file = evaluator.run_comprehensive_evaluation(max_samples=20)  # å…ˆç”¨20ä¸ªæ ·æœ¬æµ‹è¯•
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼ç»“æœæ–‡ä»¶: {output_file}")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 