#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨æœ¬åœ°Qwen3-1.7Bæ¨¡å‹è¿›è¡Œå¾®è°ƒ
åŸºäºç»æµå­¦é—®ç­”æ•°æ®é›†è®­ç»ƒç»æµå­¦é—®ç­”åŠ©æ‰‹
"""

import os
import sys
import json
import time
import torch
import logging
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    get_linear_schedule_with_warmup
)
from datasets import Dataset
from peft import LoraConfig, get_peft_model, TaskType
import numpy as np

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Qwen3EconomicsTrainer:
    def __init__(self, 
                 model_path="fine_tuning/models/qwen3-1.7b",
                 data_path="ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl",
                 output_dir="fine_tuning/qwen3_economics_model"):
        """
        åˆå§‹åŒ–Qwen3ç»æµå­¦å¾®è°ƒå™¨
        
        Args:
            model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
            data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.model_path = Path(model_path)
        self.data_path = Path(data_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥è·¯å¾„
        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {self.model_path}")
        if not self.data_path.exists():
            raise FileNotFoundError(f"æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {self.data_path}")
        
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ¨¡å‹å’Œtokenizer
        self.model = None
        self.tokenizer = None
        
    def load_model_and_tokenizer(self):
        """
        åŠ è½½æœ¬åœ°æ¨¡å‹å’Œtokenizer
        """
        logger.info(f"ä»æœ¬åœ°åŠ è½½Qwen3æ¨¡å‹: {self.model_path}")
        
        try:
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(self.model_path),
                trust_remote_code=True,
                padding_side="left"
            )
            
            # ç¡®ä¿æœ‰pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
            logger.info(f"TokenizeråŠ è½½æˆåŠŸï¼Œè¯æ±‡é‡: {self.tokenizer.vocab_size}")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModelForCausalLM.from_pretrained(
                str(self.model_path),
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‚æ•°é‡: {self.model.num_parameters():,}")
            
            # é…ç½®LoRA
            self.setup_lora()
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            raise
    
    def setup_lora(self):
        """
        é…ç½®LoRAå‚æ•°
        """
        logger.info("é…ç½®LoRAå‚æ•°...")
        
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=16,  # rank
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        logger.info("LoRAé…ç½®å®Œæˆ")
    
    def load_and_prepare_data(self):
        """
        åŠ è½½å¹¶å‡†å¤‡è®­ç»ƒæ•°æ®
        """
        logger.info(f"åŠ è½½æ•°æ®: {self.data_path}")
        
        # è¯»å–JSONLæ–‡ä»¶
        data = []
        with open(self.data_path, 'r', encoding='utf-8') as f:
            for line in f:
                item = json.loads(line.strip())
                data.append(item)
        
        logger.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        formatted_data = []
        for item in data:
            # æ„å»ºå¯¹è¯æ ¼å¼çš„æ–‡æœ¬
            conversation = f"<|user|>\n{item['question']}\n<|assistant|>\n{item['answer']}<|endoftext|>"
            formatted_data.append({"text": conversation})
        
        # åˆ†å‰²æ•°æ®
        train_size = int(0.8 * len(formatted_data))
        val_size = int(0.1 * len(formatted_data))
        
        train_data = formatted_data[:train_size]
        val_data = formatted_data[train_size:train_size + val_size]
        test_data = formatted_data[train_size + val_size:]
        
        logger.info(f"è®­ç»ƒé›†: {len(train_data)} æ¡")
        logger.info(f"éªŒè¯é›†: {len(val_data)} æ¡")
        logger.info(f"æµ‹è¯•é›†: {len(test_data)} æ¡")
        
        # åˆ›å»ºDataset
        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)
        
        # Tokenizeæ•°æ®
        def tokenize_function(examples):
            # æ­£ç¡®çš„tokenizationï¼Œç¡®ä¿è¿”å›input_idså’Œattention_mask
            tokenized = self.tokenizer(
                examples["text"],
                truncation=True,
                padding=True,  # å¯ç”¨padding
                max_length=512,
                return_overflowing_tokens=False,
                return_tensors=None,  # ä¸ç›´æ¥è¿”å›tensorï¼Œè®©DataCollatorå¤„ç†
            )
            # å¯¹äºè¯­è¨€æ¨¡å‹ï¼Œlabelså°±æ˜¯input_ids
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=["text"])
        
        return train_dataset, val_dataset, test_data
    
    def train_model(self, train_dataset, val_dataset):
        """
        è®­ç»ƒæ¨¡å‹
        """
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=str(self.output_dir),
            overwrite_output_dir=True,
            num_train_epochs=3,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            gradient_accumulation_steps=4,
            learning_rate=5e-5,
            weight_decay=0.01,
            logging_steps=10,
            save_steps=100,
            eval_steps=100,
            save_total_limit=3,
            eval_strategy="steps",  # æ–°ç‰ˆæœ¬ä½¿ç”¨eval_strategyè€Œä¸æ˜¯evaluation_strategy
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            warmup_steps=100,
            fp16=torch.cuda.is_available(),
            dataloader_drop_last=False,
            report_to=None,  # ä¸ä¸ŠæŠ¥åˆ°wandbç­‰
            remove_unused_columns=True,  # ç§»é™¤æœªä½¿ç”¨çš„åˆ—
            dataloader_num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
        )
        
        # æ•°æ®æ•´ç†å™¨
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # ä¸ä½¿ç”¨masked language modeling
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # è®­ç»ƒ
        start_time = time.time()
        trainer.train()
        end_time = time.time()
        
        logger.info(f"è®­ç»ƒå®Œæˆ! è€—æ—¶: {end_time - start_time:.1f}ç§’")
        
        # ä¿å­˜æ¨¡å‹
        trainer.save_model()
        self.tokenizer.save_pretrained(str(self.output_dir))
        
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {self.output_dir}")
        
        return trainer
    
    def test_model(self, test_data, max_samples=10):
        """
        æµ‹è¯•è®­ç»ƒåçš„æ¨¡å‹
        """
        logger.info("æµ‹è¯•æ¨¡å‹æ€§èƒ½...")
        
        # åˆ‡æ¢åˆ°æ¨ç†æ¨¡å¼
        self.model.eval()
        
        test_samples = test_data[:max_samples]
        
        results = []
        for i, item in enumerate(test_samples):
            # test_dataæ˜¯åŸå§‹æ•°æ®æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨textå­—æ®µ
            text_content = item["text"]
            parts = text_content.replace("<|user|>\n", "").replace("<|endoftext|>", "").split("\n<|assistant|>\n")
            question = parts[0].strip()
            expected_answer = parts[1].strip() if len(parts) > 1 else ""
            
            # ç”Ÿæˆå›ç­”
            input_text = f"<|user|>\n{question}\n<|assistant|>\n"
            inputs = self.tokenizer(input_text, return_tensors="pt").to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            generated_answer = generated_text.replace(input_text, "").strip()
            
            results.append({
                "question": question,
                "expected_answer": expected_answer,
                "generated_answer": generated_answer
            })
            
            logger.info(f"\né—®é¢˜ {i+1}: {question}")
            logger.info(f"æœŸæœ›å›ç­”: {expected_answer[:100]}...")
            logger.info(f"ç”Ÿæˆå›ç­”: {generated_answer[:100]}...")
            logger.info("-" * 80)
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        results_file = self.output_dir / "test_results.json"
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
        
        return results
    
    def run_full_training(self):
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒæµç¨‹
        """
        logger.info("ğŸš€ å¼€å§‹Qwen3ç»æµå­¦é—®ç­”åŠ©æ‰‹è®­ç»ƒ")
        logger.info(f"æ¨¡å‹è·¯å¾„: {self.model_path}")
        logger.info(f"æ•°æ®è·¯å¾„: {self.data_path}")
        logger.info(f"è¾“å‡ºè·¯å¾„: {self.output_dir}")
        
        try:
            # 1. åŠ è½½æ¨¡å‹
            self.load_model_and_tokenizer()
            
            # 2. å‡†å¤‡æ•°æ®
            train_dataset, val_dataset, test_data = self.load_and_prepare_data()
            
            # 3. è®­ç»ƒæ¨¡å‹
            trainer = self.train_model(train_dataset, val_dataset)
            
            # 4. æµ‹è¯•æ¨¡å‹
            results = self.test_model(test_data)
            
            # 5. ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
            self.generate_training_report(trainer, results)
            
            logger.info("âœ… è®­ç»ƒæµç¨‹å®Œæˆ!")
            
        except Exception as e:
            logger.error(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
            raise
    
    def generate_training_report(self, trainer, test_results):
        """
        ç”Ÿæˆè®­ç»ƒæŠ¥å‘Š
        """
        report = {
            "training_time": datetime.now().isoformat(),
            "model_path": str(self.model_path),
            "data_path": str(self.data_path),
            "output_dir": str(self.output_dir),
            "training_args": trainer.args.to_dict(),
            "final_train_loss": trainer.state.log_history[-1].get("train_loss", "N/A"),
            "final_eval_loss": trainer.state.log_history[-1].get("eval_loss", "N/A"),
            "test_samples": len(test_results),
            "model_info": {
                "model_type": self.model.config.model_type,
                "num_parameters": self.model.num_parameters(),
                "vocab_size": self.tokenizer.vocab_size
            }
        }
        
        report_file = self.output_dir / "training_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ¤– Qwen3ç»æµå­¦é—®ç­”åŠ©æ‰‹è®­ç»ƒ")
    print("ä½¿ç”¨æœ¬åœ°Qwen3-1.7Bæ¨¡å‹å’Œç»æµå­¦é—®ç­”æ•°æ®é›†")
    print()
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
    model_path = Path("fine_tuning/models/qwen3-1.7b")
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        print("è¯·å…ˆè¿è¡Œ download_models.py ä¸‹è½½æ¨¡å‹")
        return
    
    # æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨
    data_path = Path("ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl")
    if not data_path.exists():
        print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
        return
    
    print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹è®­ç»ƒ...")
    
    trainer = Qwen3EconomicsTrainer()
    trainer.run_full_training()

if __name__ == "__main__":
    main() 