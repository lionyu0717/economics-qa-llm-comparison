#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€ä¸ªæµ‹è¯•æ¨¡å‹çš„å¯¹æ¯”è„šæœ¬
é¿å…æ˜¾å­˜ä¸è¶³é—®é¢˜
"""

import torch
import json
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time
import gc

class SequentialModelComparator:
    def __init__(self):
        self.base_model_path = "fine_tuning/models/qwen3-1.7b"
        self.tuned_model_path = "fine_tuning/qwen3_economics_model"
        
    def clear_gpu_memory(self):
        """æ¸…ç†GPUå†…å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
    def load_base_model(self):
        """åŠ è½½åŸå§‹æ¨¡å‹"""
        print("ğŸ“¦ åŠ è½½åŸå§‹æ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        model.eval()
        
        return tokenizer, model
    
    def load_tuned_model(self):
        """åŠ è½½å¾®è°ƒæ¨¡å‹"""
        print("ğŸ¯ åŠ è½½å¾®è°ƒæ¨¡å‹...")
        
        tokenizer = AutoTokenizer.from_pretrained(self.base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        base_model = AutoModelForCausalLM.from_pretrained(
            self.base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        tuned_model = PeftModel.from_pretrained(base_model, self.tuned_model_path)
        tuned_model.eval()
        
        return tokenizer, tuned_model
    
    def generate_answer(self, model, tokenizer, question):
        """ç”Ÿæˆå›ç­”"""
        input_text = f"<|user|>\n{question}\n<|assistant|>\n"
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            start_time = time.time()
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            generation_time = time.time() - start_time
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text.replace(input_text, "").strip()
            
            # æ¸…ç†ç­”æ¡ˆ
            if "\n" in answer:
                answer = answer.split("\n")[0]
            
            return answer[:250], generation_time
            
        except Exception as e:
            return f"ç”Ÿæˆå¤±è´¥: {str(e)}", 0
    
    def get_test_questions(self):
        """è·å–æµ‹è¯•é—®é¢˜"""
        return [
            "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
            "è¯·è§£é‡Šä¾›ç»™ä¸éœ€æ±‚çš„å…³ç³»", 
            "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯å¸‚åœºå¤±çµï¼Ÿ",
            "è§£é‡Šé€šè´§è†¨èƒ€çš„å«ä¹‰",
            "ä»€ä¹ˆæ˜¯æ¯”è¾ƒä¼˜åŠ¿ï¼Ÿ"
        ]
    
    def test_model(self, model_type="base"):
        """æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        questions = self.get_test_questions()
        results = []
        
        if model_type == "base":
            tokenizer, model = self.load_base_model()
            print(f"âœ… åŸå§‹æ¨¡å‹åŠ è½½å®Œæˆ!")
        else:
            tokenizer, model = self.load_tuned_model()
            print(f"âœ… å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ!")
        
        print(f"\nğŸ“ å¼€å§‹æµ‹è¯• {model_type} æ¨¡å‹...")
        print("=" * 50)
        
        for i, question in enumerate(questions):
            print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
            print("-" * 40)
            
            answer, gen_time = self.generate_answer(model, tokenizer, question)
            print(f"ğŸ’¡ å›ç­”: {answer}")
            print(f"â±ï¸ æ—¶é—´: {gen_time:.2f}ç§’")
            
            results.append({
                "question": question,
                "answer": answer,
                "time": gen_time
            })
        
        # æ¸…ç†å†…å­˜
        del model, tokenizer
        self.clear_gpu_memory()
        
        return results
    
    def compare_models(self):
        """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
        print("ğŸ¯ é€ä¸ªæµ‹è¯•æ¨¡å‹å¯¹æ¯”")
        print("=" * 50)
        
        # æµ‹è¯•åŸå§‹æ¨¡å‹
        print("\nğŸ”„ ç¬¬1é˜¶æ®µ: æµ‹è¯•åŸå§‹æ¨¡å‹")
        base_results = self.test_model("base")
        
        print("\nâ¸ï¸ ç­‰å¾…3ç§’æ¸…ç†å†…å­˜...")
        time.sleep(3)
        
        # æµ‹è¯•å¾®è°ƒæ¨¡å‹
        print("\nğŸ”„ ç¬¬2é˜¶æ®µ: æµ‹è¯•å¾®è°ƒæ¨¡å‹")
        tuned_results = self.test_model("tuned")
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        self.create_comparison_report(base_results, tuned_results)
        
        return base_results, tuned_results
    
    def create_comparison_report(self, base_results, tuned_results):
        """åˆ›å»ºå¯¹æ¯”æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        base_avg_time = sum(r["time"] for r in base_results) / len(base_results)
        tuned_avg_time = sum(r["time"] for r in tuned_results) / len(tuned_results)
        base_avg_len = sum(len(r["answer"]) for r in base_results) / len(base_results)
        tuned_avg_len = sum(len(r["answer"]) for r in tuned_results) / len(tuned_results)
        
        # ä¿å­˜JSONç»“æœ
        comparison_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "base_results": base_results,
            "tuned_results": tuned_results,
            "summary": {
                "base_avg_time": base_avg_time,
                "tuned_avg_time": tuned_avg_time,
                "base_avg_length": base_avg_len,
                "tuned_avg_length": tuned_avg_len
            }
        }
        
        with open("sequential_comparison.json", 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, ensure_ascii=False, indent=2)
        
        # åˆ›å»ºå¯è¯»æŠ¥å‘Š
        with open("model_comparison_sequential.txt", 'w', encoding='utf-8') as f:
            f.write("ğŸ¯ Qwen3 ç»æµå­¦æ¨¡å‹å¾®è°ƒå‰åå¯¹æ¯”æŠ¥å‘Š\n")
            f.write("=" * 60 + "\n\n")
            f.write(f"è¯„ä¼°æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æµ‹è¯•é—®é¢˜æ•°é‡: {len(base_results)}\n\n")
            
            for i, (base, tuned) in enumerate(zip(base_results, tuned_results)):
                f.write(f"é—®é¢˜ {i+1}: {base['question']}\n")
                f.write("-" * 50 + "\n")
                f.write(f"åŸå§‹æ¨¡å‹ ({base['time']:.2f}s): {base['answer']}\n\n")
                f.write(f"å¾®è°ƒæ¨¡å‹ ({tuned['time']:.2f}s): {tuned['answer']}\n\n")
                f.write("=" * 60 + "\n\n")
            
            # ç»Ÿè®¡æ€»ç»“
            f.write("ğŸ“Š å¯¹æ¯”æ€»ç»“:\n")
            f.write("-" * 30 + "\n")
            f.write(f"åŸå§‹æ¨¡å‹å¹³å‡å›ç­”æ—¶é—´: {base_avg_time:.2f}ç§’\n")
            f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡å›ç­”æ—¶é—´: {tuned_avg_time:.2f}ç§’\n")
            f.write(f"åŸå§‹æ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {base_avg_len:.1f}å­—ç¬¦\n")
            f.write(f"å¾®è°ƒæ¨¡å‹å¹³å‡å›ç­”é•¿åº¦: {tuned_avg_len:.1f}å­—ç¬¦\n\n")
            
            # ç®€å•è¯„ä»·
            if tuned_avg_len > base_avg_len * 1.2:
                f.write("âœ… å¾®è°ƒæ¨¡å‹å›ç­”æ›´è¯¦ç»†ï¼Œå¯èƒ½å­¦åˆ°äº†æ›´å¤šç»æµå­¦çŸ¥è¯†\n")
            elif tuned_avg_len < base_avg_len * 0.8:
                f.write("âš ï¸ å¾®è°ƒæ¨¡å‹å›ç­”è¾ƒçŸ­ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°\n")
            else:
                f.write("ğŸ” éœ€è¦äººå·¥è¯„ä¼°å›ç­”è´¨é‡æ¥åˆ¤æ–­å¾®è°ƒæ•ˆæœ\n")
        
        # æ‰“å°æ€»ç»“
        print("\nğŸ‰ å¯¹æ¯”è¯„ä¼°å®Œæˆ!")
        print("=" * 40)
        print(f"ğŸ“Š æµ‹è¯•é—®é¢˜: {len(base_results)} ä¸ª")
        print(f"ğŸ“¦ åŸå§‹æ¨¡å‹å¹³å‡ç”¨æ—¶: {base_avg_time:.2f}ç§’")
        print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹å¹³å‡ç”¨æ—¶: {tuned_avg_time:.2f}ç§’")
        print(f"ğŸ“¦ åŸå§‹æ¨¡å‹å¹³å‡é•¿åº¦: {base_avg_len:.1f}å­—ç¬¦")
        print(f"ğŸ¯ å¾®è°ƒæ¨¡å‹å¹³å‡é•¿åº¦: {tuned_avg_len:.1f}å­—ç¬¦")
        
        print(f"\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: model_comparison_sequential.txt")
        print(f"ğŸ“„ JSONæ•°æ®å·²ä¿å­˜åˆ°: sequential_comparison.json")
        
        # æ˜¾ç¤ºä¸¤ä¸ªå…·ä½“ä¾‹å­
        print(f"\nğŸ” å¯¹æ¯”ç¤ºä¾‹:")
        print("=" * 40)
        for i in range(min(2, len(base_results))):
            print(f"\nâ“ {base_results[i]['question']}")
            print(f"ğŸ“¦ åŸå§‹: {base_results[i]['answer'][:100]}...")
            print(f"ğŸ¯ å¾®è°ƒ: {tuned_results[i]['answer'][:100]}...")

def main():
    print("ğŸ¯ é€ä¸ªæ¨¡å‹å¯¹æ¯”è¯„ä¼°")
    print("è§£å†³æ˜¾å­˜ä¸è¶³é—®é¢˜ï¼Œé€ä¸ªæµ‹è¯•æ¨¡å‹")
    print("=" * 50)
    
    comparator = SequentialModelComparator()
    
    try:
        base_results, tuned_results = comparator.compare_models()
        print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼å…±æµ‹è¯•äº† {len(base_results)} ä¸ªé—®é¢˜")
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 