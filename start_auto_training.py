#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
è‡ªåŠ¨å¯åŠ¨è®­ç»ƒè„šæœ¬
è‡ªåŠ¨å¼€å§‹QWENå’ŒLLAMAæ¨¡å‹çš„å¾®è°ƒè®­ç»ƒ
"""

import subprocess
import sys
import os
import time
from datetime import datetime

def start_qwen_training():
    """å¯åŠ¨QWENè®­ç»ƒ"""
    print("="*50)
    print("ğŸš€ å¯åŠ¨QWENæ¨¡å‹å¾®è°ƒ")
    print("="*50)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("æ¨¡å‹: Qwen2-1.5B-Instruct")
    print("æ•°æ®: 100ä¸ªç»æµå­¦é—®ç­”å¯¹ï¼ˆå¿«é€Ÿè®­ç»ƒï¼‰")
    print("æ–¹æ³•: LoRAå¾®è°ƒ")
    print()
    
    try:
        result = subprocess.run([
            sys.executable, "fine_tuning/scripts/train_qwen_simple.py"
        ], capture_output=False, text=True, cwd=".")
        
        return result.returncode == 0
    except Exception as e:
        print(f"QWENè®­ç»ƒå¤±è´¥: {e}")
        return False

def start_llama_training():
    """å¯åŠ¨LLAMAè®­ç»ƒ"""
    print("="*50)
    print("ğŸš€ å¯åŠ¨LLAMAæ¨¡å‹å¾®è°ƒ")
    print("="*50)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("æ¨¡å‹: DialoGPT-medium (ä½œä¸ºLLAMAæ›¿ä»£)")
    print("æ•°æ®: 100ä¸ªç»æµå­¦é—®ç­”å¯¹ï¼ˆå¿«é€Ÿè®­ç»ƒï¼‰")
    print("æ–¹æ³•: LoRAå¾®è°ƒ")
    print()
    
    try:
        result = subprocess.run([
            sys.executable, "fine_tuning/scripts/train_llama_simple.py"
        ], capture_output=False, text=True, cwd=".")
        
        return result.returncode == 0
    except Exception as e:
        print(f"LLAMAè®­ç»ƒå¤±è´¥: {e}")
        return False

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒ"""
    print("ğŸ” æ£€æŸ¥è®­ç»ƒç¯å¢ƒ...")
    
    # æ£€æŸ¥GPU
    try:
        import torch
        gpu_available = torch.cuda.is_available()
        print(f"GPUå¯ç”¨: {'âœ…' if gpu_available else 'âŒ'}")
        if gpu_available:
            print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
    except ImportError:
        print("âŒ PyTorchæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥æ•°æ®
    data_files = [
        "fine_tuning/data/qwen/train.jsonl",
        "fine_tuning/data/alpaca/train.json"
    ]
    
    for file_path in data_files:
        if os.path.exists(file_path):
            print(f"âœ… æ•°æ®æ–‡ä»¶å­˜åœ¨: {file_path}")
        else:
            print(f"âŒ æ•°æ®æ–‡ä»¶ç¼ºå¤±: {file_path}")
            return False
    
    return True

def monitor_progress():
    """ç›‘æ§è®­ç»ƒè¿›åº¦"""
    while True:
        os.system('cls' if os.name == 'nt' else 'clear')
        print("ğŸ“Š è®­ç»ƒè¿›åº¦ç›‘æ§")
        print("="*40)
        print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        # æ£€æŸ¥QWENè¿›åº¦
        qwen_checkpoint = "fine_tuning/qwen/checkpoints"
        if os.path.exists(qwen_checkpoint):
            qwen_files = [f for f in os.listdir(qwen_checkpoint) if f.endswith('.bin') or f.endswith('.safetensors')]
            print(f"QWEN: âœ… {len(qwen_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶")
        else:
            print("QWEN: â³ è®­ç»ƒä¸­...")
        
        # æ£€æŸ¥LLAMAè¿›åº¦
        llama_checkpoint = "fine_tuning/llama/checkpoints"
        if os.path.exists(llama_checkpoint):
            llama_files = [f for f in os.listdir(llama_checkpoint) if f.endswith('.bin') or f.endswith('.safetensors')]
            print(f"LLAMA: âœ… {len(llama_files)} ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶")
        else:
            print("LLAMA: â³ è®­ç»ƒä¸­...")
        
        print("\næŒ‰ Ctrl+C åœæ­¢ç›‘æ§")
        time.sleep(10)

def main():
    print("ğŸ¯ ç»æµå­¦é—®ç­”åŠ©æ‰‹æ¨¡å‹å¾®è°ƒå¯åŠ¨å™¨")
    print("="*60)
    
    # æ£€æŸ¥ç¯å¢ƒ
    if not check_environment():
        print("âŒ ç¯å¢ƒæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¾èµ–å’Œæ•°æ®")
        return
    
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    print()
    
    # å¯åŠ¨QWENè®­ç»ƒ
    print("1ï¸âƒ£ å¼€å§‹QWENæ¨¡å‹å¾®è°ƒ...")
    qwen_success = start_qwen_training()
    
    if qwen_success:
        print("âœ… QWENè®­ç»ƒå®Œæˆ")
    else:
        print("âŒ QWENè®­ç»ƒå¤±è´¥")
    
    print("\n" + "="*50)
    print("ç­‰å¾…5ç§’åå¼€å§‹LLAMAè®­ç»ƒ...")
    time.sleep(5)
    
    # å¯åŠ¨LLAMAè®­ç»ƒ
    print("2ï¸âƒ£ å¼€å§‹LLAMAæ¨¡å‹å¾®è°ƒ...")
    llama_success = start_llama_training()
    
    if llama_success:
        print("âœ… LLAMAè®­ç»ƒå®Œæˆ")
    else:
        print("âŒ LLAMAè®­ç»ƒå¤±è´¥")
    
    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ è®­ç»ƒæ€»ç»“")
    print("="*60)
    print(f"QWENæ¨¡å‹: {'âœ… æˆåŠŸ' if qwen_success else 'âŒ å¤±è´¥'}")
    print(f"LLAMAæ¨¡å‹: {'âœ… æˆåŠŸ' if llama_success else 'âŒ å¤±è´¥'}")
    
    if qwen_success or llama_success:
        print("\nğŸ‰ è‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹è®­ç»ƒæˆåŠŸï¼")
        print("æ¥ä¸‹æ¥å¯ä»¥è¿è¡Œæ¨¡å‹æµ‹è¯•:")
        print("python fine_tuning/scripts/test_simple.py")
    else:
        print("\nğŸ˜ æ‰€æœ‰æ¨¡å‹è®­ç»ƒéƒ½å¤±è´¥äº†ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­è®­ç»ƒ")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc() 