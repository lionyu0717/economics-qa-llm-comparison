#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•å·²è®­ç»ƒå®Œæˆçš„Qwen3ç»æµå­¦æ¨¡å‹
"""

import torch
import json
from pathlib import Path
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def load_trained_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("ğŸ¤– åŠ è½½Qwen3ç»æµå­¦é—®ç­”æ¨¡å‹...")
    
    base_model_path = "fine_tuning/models/qwen3-1.7b"
    trained_model_path = "fine_tuning/qwen3_economics_model"
    
    # æ£€æŸ¥è·¯å¾„
    if not Path(base_model_path).exists():
        print(f"âŒ åŸºç¡€æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {base_model_path}")
        return None, None
    
    if not Path(trained_model_path).exists():
        print(f"âŒ è®­ç»ƒæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {trained_model_path}")
        return None, None
    
    try:
        # åŠ è½½tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        
        # åŠ è½½LoRAæƒé‡
        model = PeftModel.from_pretrained(base_model, trained_model_path)
        model.eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model, tokenizer
    
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None

def test_economics_questions(model, tokenizer):
    """æµ‹è¯•ç»æµå­¦é—®é¢˜"""
    
    # å‡†å¤‡æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯ç¨€ç¼ºæ€§ï¼Ÿ",
        "è¯·è§£é‡Šä¾›ç»™ä¸éœ€æ±‚çš„å…³ç³»",
        "ä»€ä¹ˆæ˜¯æœºä¼šæˆæœ¬ï¼Ÿ",
        "è¯·è¯´æ˜ä»·æ ¼å¼¹æ€§çš„æ¦‚å¿µ",
        "ä»€ä¹ˆæ˜¯å¸‚åœºå¤±çµï¼Ÿ",
        "è§£é‡Šé€šè´§è†¨èƒ€çš„å«ä¹‰",
        "ä»€ä¹ˆæ˜¯GDPï¼Ÿ",
        "è¯·è¯´æ˜å„æ–­å¸‚åœºçš„ç‰¹ç‚¹"
    ]
    
    print("ğŸ“ å¼€å§‹æµ‹è¯•ç»æµå­¦é—®ç­”...")
    print("=" * 60)
    
    results = []
    
    for i, question in enumerate(test_questions):
        print(f"\nâ“ é—®é¢˜ {i+1}: {question}")
        print("-" * 40)
        
        # æ„å»ºè¾“å…¥
        input_text = f"<|user|>\n{question}\n<|assistant|>\n"
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        try:
            # ç”Ÿæˆå›ç­”
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    repetition_penalty=1.1
                )
            
            # è§£ç è¾“å‡º
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = generated_text.replace(input_text, "").strip()
            
            print(f"ğŸ’¡ å›ç­”: {answer}")
            
            results.append({
                "question": question,
                "answer": answer
            })
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            results.append({
                "question": question,
                "answer": f"ç”Ÿæˆå¤±è´¥: {e}"
            })
    
    return results

def save_test_results(results):
    """ä¿å­˜æµ‹è¯•ç»“æœ"""
    output_file = "qwen3_economics_test_results.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ Qwen3ç»æµå­¦é—®ç­”æ¨¡å‹æµ‹è¯•")
    print("=" * 50)
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = load_trained_model()
    if model is None:
        return
    
    # æµ‹è¯•é—®é¢˜
    results = test_economics_questions(model, tokenizer)
    
    # ä¿å­˜ç»“æœ
    save_test_results(results)
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print(f"æ€»å…±æµ‹è¯•äº† {len(results)} ä¸ªé—®é¢˜")
    
    # æ˜¾ç¤ºGPUçŠ¶æ€
    if torch.cuda.is_available():
        memory_used = torch.cuda.memory_allocated() / 1024**3
        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"ğŸ® GPUæ˜¾å­˜ä½¿ç”¨: {memory_used:.1f}/{memory_total:.1f} GB")

if __name__ == "__main__":
    main() 