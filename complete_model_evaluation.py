#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„ç»æµå­¦é—®ç­”æ¨¡å‹è¯„ä¼°
åŒ…å«æ ‡å‡†çš„NLPè¯„ä»·æŒ‡æ ‡ï¼Œä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œæ€§èƒ½è¯„ä¼°
"""

import torch
import json
import time
import numpy as np
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import logging
from datetime import datetime

# NLPè¯„ä¼°æŒ‡æ ‡
try:
    from rouge_score import rouge_scorer
    from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
    import jieba
    METRICS_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: éƒ¨åˆ†è¯„ä¼°æŒ‡æ ‡åº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨åŸºç¡€æŒ‡æ ‡")
    METRICS_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveModelEvaluator:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # æ¨¡å‹é…ç½® 
        self.models_config = {
            "qwen3": {
                "base_model": "fine_tuning/models/qwen3-1.7b",
                "tuned_model": "fine_tuning/qwen3_economics_model",
                "display_name": "Qwen3-1.7B"
            },
            "gemma3": {
                "base_model": "google/gemma-3-1b-it", 
                "tuned_model": "fine_tuning/gemma3_economics_model",
                "display_name": "Gemma3-1B"
            }
        }
        
        # æµ‹è¯•æ•°æ®è·¯å¾„
        self.test_data_path = "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
        
        # è¯„ä¼°æŒ‡æ ‡åˆå§‹åŒ–
        if METRICS_AVAILABLE:
            self.rouge_scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=False)
            self.smoothing = SmoothingFunction().method1
        
    def load_test_data(self, max_samples=50):
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        logger.info("ğŸ“š åŠ è½½æµ‹è¯•æ•°æ®...")
        
        test_data = []
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            all_data = [json.loads(line) for line in f]
        
        # å–æœ€åçš„æµ‹è¯•æ ·æœ¬ï¼ˆå’Œè®­ç»ƒæ—¶çš„åˆ†å‰²ä¿æŒä¸€è‡´ï¼‰
        total_samples = len(all_data)
        test_start = int(total_samples * 0.9)  # å‰90%æ˜¯è®­ç»ƒ+éªŒè¯ï¼Œå10%æ˜¯æµ‹è¯•
        test_samples = all_data[test_start:test_start + max_samples]
        
        for item in test_samples:
            test_data.append({
                "question": item["question"],
                "reference": item["answer"]
            })
        
        logger.info(f"åŠ è½½äº† {len(test_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
        return test_data
    
    def load_model(self, model_name):
        """åŠ è½½å¾®è°ƒæ¨¡å‹"""
        try:
            config = self.models_config[model_name]
            logger.info(f"ğŸ”„ åŠ è½½ {config['display_name']} æ¨¡å‹...")
            
            # åŠ è½½tokenizer
            tokenizer = AutoTokenizer.from_pretrained(config["base_model"], trust_remote_code=True)
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                config["base_model"],
                torch_dtype=torch.bfloat16,
                device_map="auto", 
                trust_remote_code=True,
                load_in_8bit=True
            )
            
            # åŠ è½½LoRAé€‚é…å™¨
            model = PeftModel.from_pretrained(base_model, config["tuned_model"])
            model.eval()
            
            logger.info(f"âœ… {config['display_name']} æ¨¡å‹åŠ è½½æˆåŠŸ")
            return model, tokenizer
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½ {config['display_name']} æ¨¡å‹å¤±è´¥: {e}")
            return None, None
    
    def generate_answer_optimized(self, model, tokenizer, question, model_name):
        """ä½¿ç”¨ä¼˜åŒ–å‚æ•°ç”Ÿæˆå›ç­”ï¼ˆnon-thinkingæ¨¡å¼ï¼‰"""
        try:
            if model_name == "qwen3":
                # Qwen3 non-thinkingæ¨¡å¼  
                prompt = f"<|user|>\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n{question}\n<|assistant|>\n"
                
                # å®˜æ–¹æ¨èçš„non-thinkingå‚æ•°
                generation_params = {
                    "max_new_tokens": 200,
                    "temperature": 0.7,
                    "top_p": 0.8,
                    "top_k": 20,
                    "do_sample": True,
                    "pad_token_id": tokenizer.pad_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                    "repetition_penalty": 1.3,  # é˜²æ­¢é‡å¤
                }
                
            else:  # gemma3
                # Gemma3ä½¿ç”¨èŠå¤©æ¨¡æ¿
                messages = [
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": question}
                ]
                
                try:
                    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
                except:
                    # å¤‡ç”¨æ ¼å¼
                    prompt = f"<bos><start_of_turn>user\nä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç»æµå­¦é—®ç­”åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç»æµå­¦åŸç†å‡†ç¡®å›ç­”é—®é¢˜ã€‚\n{question}<end_of_turn>\n<start_of_turn>model\n"
                
                generation_params = {
                    "max_new_tokens": 200,
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "top_k": 40,
                    "do_sample": True,
                    "pad_token_id": tokenizer.eos_token_id,
                    "eos_token_id": tokenizer.eos_token_id,
                    "repetition_penalty": 1.2,
                }
            
            # ç¼–ç è¾“å…¥
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            # ç”Ÿæˆå›ç­”
            start_time = time.time()
            with torch.inference_mode():
                outputs = model.generate(**inputs, **generation_params)
            generation_time = time.time() - start_time
            
            # è§£ç è¾“å‡º
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = full_response.replace(prompt, "").strip()
            
            # æ¸…ç†ç­”æ¡ˆ
            if "\n" in answer:
                lines = answer.split("\n")
                for line in lines:
                    if line.strip():
                        answer = line.strip()
                        break
            
            # é™åˆ¶é•¿åº¦å¹¶ç§»é™¤å¯èƒ½çš„æ ¼å¼æ ‡è®°
            answer = answer.replace("<end_of_turn>", "").strip()
            if len(answer) > 300:
                answer = answer[:300] + "..."
            
            return answer, generation_time
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {e}")
            return f"[ç”Ÿæˆå¤±è´¥: {str(e)}]", 0
    
    def calculate_rouge_scores(self, reference, candidate):
        """è®¡ç®—ROUGEåˆ†æ•°"""
        if not METRICS_AVAILABLE:
            return {"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0}
        
        try:
            scores = self.rouge_scorer.score(reference, candidate)
            return {
                'rouge1_f': scores['rouge1'].fmeasure,
                'rouge2_f': scores['rouge2'].fmeasure,
                'rougeL_f': scores['rougeL'].fmeasure,
            }
        except:
            return {"rouge1_f": 0.0, "rouge2_f": 0.0, "rougeL_f": 0.0}
    
    def calculate_bleu_score(self, reference, candidate):
        """è®¡ç®—BLEUåˆ†æ•°"""
        if not METRICS_AVAILABLE:
            return 0.0
        
        try:
            # ä½¿ç”¨jiebaåˆ†è¯
            ref_tokens = list(jieba.cut(reference))
            cand_tokens = list(jieba.cut(candidate))
            
            if len(cand_tokens) == 0:
                return 0.0
            
            return sentence_bleu([ref_tokens], cand_tokens, smoothing_function=self.smoothing)
        except:
            return 0.0
    
    def calculate_basic_similarity(self, reference, candidate):
        """è®¡ç®—åŸºç¡€ç›¸ä¼¼åº¦ï¼ˆè¯æ±‡é‡å ï¼‰"""
        try:
            if METRICS_AVAILABLE:
                ref_words = set(jieba.cut(reference))
                cand_words = set(jieba.cut(candidate))
            else:
                ref_words = set(reference.split())
                cand_words = set(candidate.split())
            
            if len(ref_words) == 0 and len(cand_words) == 0:
                return 1.0
            
            intersection = len(ref_words & cand_words)
            union = len(ref_words | cand_words)
            
            return intersection / union if union > 0 else 0.0
        except:
            return 0.0
    
    def assess_answer_quality(self, question, reference, candidate):
        """è¯„ä¼°å›ç­”è´¨é‡"""
        try:
            # 1. é•¿åº¦åˆç†æ€§
            ref_len = len(reference)
            cand_len = len(candidate)
            length_ratio = min(cand_len / max(ref_len, 1), 2.0) if cand_len > 0 else 0.0
            length_score = 1.0 - abs(1.0 - length_ratio) if length_ratio <= 2.0 else 0.0
            
            # 2. å…³é”®è¯è¦†ç›–åº¦
            if METRICS_AVAILABLE:
                question_keywords = set(jieba.cut(question))
                answer_keywords = set(jieba.cut(candidate))
            else:
                question_keywords = set(question.split())
                answer_keywords = set(candidate.split())
                
            keyword_coverage = len(question_keywords & answer_keywords) / max(len(question_keywords), 1)
            
            # 3. æµç•…æ€§æ£€æŸ¥
            fluency = 1.0
            if "ç”Ÿæˆå¤±è´¥" in candidate or len(candidate) < 5:
                fluency = 0.0
            elif len(candidate) > 50 and candidate.count("ã€‚") == 0:
                fluency = 0.7
            
            # 4. é‡å¤æ£€æŸ¥
            repetition_score = 1.0
            sentences = candidate.split("ã€‚")
            if len(sentences) > 2:
                unique_sentences = set(sentences)
                repetition_score = len(unique_sentences) / len(sentences)
            
            overall_quality = (length_score + keyword_coverage + fluency + repetition_score) / 4
            
            return {
                'length_score': length_score,
                'keyword_coverage': keyword_coverage,
                'fluency': fluency,
                'repetition_score': repetition_score,
                'overall_quality': overall_quality
            }
        except:
            return {k: 0.0 for k in ['length_score', 'keyword_coverage', 'fluency', 'repetition_score', 'overall_quality']}
    
    def evaluate_model(self, model_name, test_data):
        """è¯„ä¼°å•ä¸ªæ¨¡å‹"""
        logger.info(f"ğŸ“Š è¯„ä¼° {self.models_config[model_name]['display_name']} æ¨¡å‹...")
        
        # åŠ è½½æ¨¡å‹
        model, tokenizer = self.load_model(model_name)
        if model is None:
            return None
        
        results = []
        total_time = 0
        
        for i, item in enumerate(test_data):
            question = item["question"]
            reference = item["reference"]
            
            logger.info(f"ğŸ”¸ å¤„ç†é—®é¢˜ {i+1}/{len(test_data)}: {question[:50]}...")
            
            # ç”Ÿæˆå›ç­”
            candidate, gen_time = self.generate_answer_optimized(model, tokenizer, question, model_name)
            total_time += gen_time
            
            # è®¡ç®—å„ç§è¯„ä»·æŒ‡æ ‡
            rouge_scores = self.calculate_rouge_scores(reference, candidate)
            bleu_score = self.calculate_bleu_score(reference, candidate)
            basic_sim = self.calculate_basic_similarity(reference, candidate)
            quality_scores = self.assess_answer_quality(question, reference, candidate)
            
            result = {
                "question": question,
                "reference": reference,
                "candidate": candidate,
                "generation_time": gen_time,
                "answer_length": len(candidate),
                **rouge_scores,
                "bleu": bleu_score,
                "basic_similarity": basic_sim,
                **quality_scores
            }
            results.append(result)
            
            logger.info(f"   â±ï¸ è€—æ—¶: {gen_time:.2f}s")
            logger.info(f"   ğŸ“ å›ç­”: {candidate[:100]}...")
        
        # æ¸…ç†GPUå†…å­˜
        del model, tokenizer
        torch.cuda.empty_cache()
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_metrics = {}
        numeric_fields = ['generation_time', 'answer_length', 'rouge1_f', 'rouge2_f', 'rougeL_f', 
                         'bleu', 'basic_similarity', 'overall_quality']
        
        for field in numeric_fields:
            values = [r[field] for r in results if isinstance(r[field], (int, float))]
            avg_metrics[f"avg_{field}"] = np.mean(values) if values else 0.0
        
        model_stats = {
            "model_name": self.models_config[model_name]['display_name'],
            "total_samples": len(test_data),
            "results": results,
            **avg_metrics
        }
        
        logger.info(f"âœ… {self.models_config[model_name]['display_name']} è¯„ä¼°å®Œæˆ")
        logger.info(f"   ğŸ“Š å¹³å‡ROUGE-1: {avg_metrics['avg_rouge1_f']:.4f}")
        logger.info(f"   ğŸ“Š å¹³å‡BLEU: {avg_metrics['avg_bleu']:.4f}")
        logger.info(f"   ğŸ“Š å¹³å‡ç›¸ä¼¼åº¦: {avg_metrics['avg_basic_similarity']:.4f}")
        logger.info(f"   ğŸ“Š å¹³å‡è´¨é‡è¯„åˆ†: {avg_metrics['avg_overall_quality']:.4f}")
        
        return model_stats
    
    def generate_comprehensive_report(self, qwen_stats, gemma_stats):
        """ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"comprehensive_evaluation_report_{timestamp}.txt"
        
        report = []
        report.append("ğŸ¯ ç»æµå­¦é—®ç­”æ¨¡å‹ç»¼åˆè¯„ä¼°æŠ¥å‘Š")
        report.append("=" * 60)
        report.append(f"è¯„ä¼°æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"æµ‹è¯•æ ·æœ¬æ•°é‡: {qwen_stats['total_samples']}")
        report.append("")
        
        # è¯„ä»·æŒ‡æ ‡è¯´æ˜
        report.append("ğŸ“Š è¯„ä»·æŒ‡æ ‡è¯´æ˜:")
        report.append("-" * 40)
        report.append("â€¢ ROUGE-1: è¯æ±‡çº§åˆ«é‡å åº¦ (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ ROUGE-2: åŒè¯ç»„åˆé‡å åº¦ (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ ROUGE-L: æœ€é•¿å…¬å…±å­åºåˆ—é‡å åº¦ (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ BLEU: æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡ (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ åŸºç¡€ç›¸ä¼¼åº¦: è¯æ±‡é‡å åº¦ (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ ç»¼åˆè´¨é‡è¯„åˆ†: å¤šç»´åº¦è´¨é‡è¯„ä¼° (å€¼è¶Šé«˜è¶Šå¥½)")
        report.append("â€¢ ç”Ÿæˆæ—¶é—´: æ¨¡å‹å“åº”é€Ÿåº¦ (å€¼è¶Šä½è¶Šå¥½)")
        report.append("")
        
        # ä¸»è¦æŒ‡æ ‡å¯¹æ¯”
        report.append("ğŸ† ä¸»è¦æŒ‡æ ‡å¯¹æ¯”:")
        report.append("-" * 40)
        
        metrics = [
            ("ROUGE-1 F1", "avg_rouge1_f", "é«˜"),
            ("ROUGE-2 F1", "avg_rouge2_f", "é«˜"), 
            ("ROUGE-L F1", "avg_rougeL_f", "é«˜"),
            ("BLEUè¯„åˆ†", "avg_bleu", "é«˜"),
            ("åŸºç¡€ç›¸ä¼¼åº¦", "avg_basic_similarity", "é«˜"),
            ("ç»¼åˆè´¨é‡", "avg_overall_quality", "é«˜"),
            ("å¹³å‡ç”Ÿæˆæ—¶é—´(s)", "avg_generation_time", "ä½"),
            ("å¹³å‡å›ç­”é•¿åº¦", "avg_answer_length", "-")
        ]
        
        for metric_name, field, better in metrics:
            qwen_val = qwen_stats[field]
            gemma_val = gemma_stats[field]
            
            if better == "é«˜":
                winner = "Qwen3" if qwen_val > gemma_val else "Gemma3"
                improvement = abs(qwen_val - gemma_val) / max(min(qwen_val, gemma_val), 0.001) * 100
            elif better == "ä½":
                winner = "Qwen3" if qwen_val < gemma_val else "Gemma3"
                improvement = abs(qwen_val - gemma_val) / max(max(qwen_val, gemma_val), 0.001) * 100
            else:
                winner = "-"
                improvement = 0
            
            report.append(f"{metric_name}:")
            report.append(f"  Qwen3:  {qwen_val:.4f}")
            report.append(f"  Gemma3: {gemma_val:.4f}")
            if winner != "-":
                report.append(f"  ä¼˜èƒœè€…: {winner} (é¢†å…ˆ {improvement:.1f}%)")
            report.append("")
        
        # è¯¦ç»†æ ·æœ¬å¯¹æ¯”ï¼ˆæ˜¾ç¤ºå‰5ä¸ªï¼‰
        report.append("ğŸ” è¯¦ç»†æ ·æœ¬å¯¹æ¯” (å‰5ä¸ª):")
        report.append("-" * 50)
        
        for i in range(min(5, len(qwen_stats["results"]))):
            qwen_result = qwen_stats["results"][i]
            gemma_result = gemma_stats["results"][i]
            
            report.append(f"\né—®é¢˜ {i+1}: {qwen_result['question']}")
            report.append(f"å‚è€ƒç­”æ¡ˆ: {qwen_result['reference'][:100]}...")
            report.append("")
            report.append(f"ğŸŸ¦ Qwen3å›ç­” (ROUGE-1: {qwen_result['rouge1_f']:.3f}, BLEU: {qwen_result['bleu']:.3f}):")
            report.append(f"   {qwen_result['candidate'][:150]}...")
            report.append("")
            report.append(f"ğŸŸ¨ Gemma3å›ç­” (ROUGE-1: {gemma_result['rouge1_f']:.3f}, BLEU: {gemma_result['bleu']:.3f}):")
            report.append(f"   {gemma_result['candidate'][:150]}...")
            report.append("-" * 40)
        
        # è¯„ä¼°ç»“è®º
        report.append("\nğŸ‰ è¯„ä¼°ç»“è®º:")
        report.append("-" * 40)
        
        qwen_wins = 0
        gemma_wins = 0
        
        for _, field, better in metrics[:6]:  # å‰6ä¸ªè´¨é‡æŒ‡æ ‡
            qwen_val = qwen_stats[field]
            gemma_val = gemma_stats[field]
            if better == "é«˜" and qwen_val > gemma_val:
                qwen_wins += 1
            elif better == "é«˜" and gemma_val > qwen_val:
                gemma_wins += 1
        
        if qwen_wins > gemma_wins:
            report.append("â€¢ æ€»ä½“è¡¨ç°: Qwen3åœ¨å¤§å¤šæ•°è´¨é‡æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½")
        elif gemma_wins > qwen_wins:
            report.append("â€¢ æ€»ä½“è¡¨ç°: Gemma3åœ¨å¤§å¤šæ•°è´¨é‡æŒ‡æ ‡ä¸Šè¡¨ç°æ›´å¥½")
        else:
            report.append("â€¢ æ€»ä½“è¡¨ç°: ä¸¤ä¸ªæ¨¡å‹å„æœ‰ä¼˜åŠ¿")
        
        speed_winner = "Qwen3" if qwen_stats["avg_generation_time"] < gemma_stats["avg_generation_time"] else "Gemma3"
        report.append(f"â€¢ é€Ÿåº¦è¡¨ç°: {speed_winner} å“åº”æ›´å¿«")
        
        # ä¿å­˜æŠ¥å‘Š
        report_content = "\n".join(report)
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        logger.info(f"ğŸ“„ ç»¼åˆè¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file
    
    def run_evaluation(self, max_samples=30):
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        logger.info("ğŸš€ å¼€å§‹ç»¼åˆæ¨¡å‹è¯„ä¼°...")
        print("\nğŸ¯ ç»æµå­¦é—®ç­”æ¨¡å‹ç»¼åˆè¯„ä¼°")
        print("åŒ…å«æ ‡å‡†NLPè¯„ä»·æŒ‡æ ‡çš„æµ‹è¯•é›†æ€§èƒ½è¯„ä¼°")
        print("=" * 60)
        
        try:
            # åŠ è½½æµ‹è¯•æ•°æ®
            test_data = self.load_test_data(max_samples)
            
            # è¯„ä¼°Qwen3
            print(f"\nğŸ”µ è¯„ä¼° Qwen3-1.7B æ¨¡å‹ (Non-thinkingæ¨¡å¼)...")
            qwen_stats = self.evaluate_model("qwen3", test_data)
            if qwen_stats is None:
                logger.error("Qwen3æ¨¡å‹è¯„ä¼°å¤±è´¥")
                return
            
            # è¯„ä¼°Gemma3
            print(f"\nğŸŸ¡ è¯„ä¼° Gemma3-1B æ¨¡å‹...")
            gemma_stats = self.evaluate_model("gemma3", test_data)
            if gemma_stats is None:
                logger.error("Gemma3æ¨¡å‹è¯„ä¼°å¤±è´¥")
                return
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            print(f"\nğŸ“ ç”Ÿæˆç»¼åˆè¯„ä¼°æŠ¥å‘Š...")
            report_file = self.generate_comprehensive_report(qwen_stats, gemma_stats)
            
            logger.info("ğŸ‰ ç»¼åˆæ¨¡å‹è¯„ä¼°å®Œæˆï¼")
            return report_file
            
        except Exception as e:
            logger.error(f"âŒ è¯„ä¼°è¿‡ç¨‹å¤±è´¥: {e}")
            raise

def main():
    print("ğŸ¯ ç»æµå­¦é—®ç­”æ¨¡å‹ç»¼åˆè¯„ä¼°")
    print("ä½¿ç”¨æ ‡å‡†NLPè¯„ä»·æŒ‡æ ‡å¯¹æµ‹è¯•é›†è¿›è¡Œæ€§èƒ½è¯„ä¼°")
    print("=" * 60)
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_paths = [
        "fine_tuning/qwen3_economics_model",
        "fine_tuning/gemma3_economics_model", 
        "ç»æµå­¦åŸç† (N.æ ¼é‡Œé«˜åˆ©æ›¼æ˜†) (Z-Library)_dataset_openrouter/qa_dataset_1000/qa_pairs_1000.jsonl"
    ]
    
    for path in required_paths:
        if not Path(path).exists():
            print(f"âŒ ç¼ºå°‘å¿…è¦æ–‡ä»¶: {path}")
            return
    
    print("âœ… æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥é€šè¿‡")
    
    # å¼€å§‹è¯„ä¼°
    evaluator = ComprehensiveModelEvaluator()
    report_file = evaluator.run_evaluation(max_samples=30)  # ä½¿ç”¨30ä¸ªæµ‹è¯•æ ·æœ¬
    
    print(f"\nğŸŠ è¯„ä¼°å®Œæˆï¼æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š: {report_file}")

if __name__ == "__main__":
    main() 